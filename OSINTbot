import os
import discord
import asyncio
import torch
import time
import requests
import gc
import psutil
import json
import re
from pathlib import Path
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    logging
)

# ─── Performance Optimizations ────────────────────────────────────────────────
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["MKL_NUM_THREADS"] = "1"
torch.set_num_threads(psutil.cpu_count(logical=False))
logging.set_verbosity_error()

print("🤖 DeepSeek-R1 High-Performance Discord Bot")
print("=" * 50)

# ─── Dependencies ──────────────────────────────────────────────────────────────
try:
    from duckduckgo_search import DDGS
    print("✅ DuckDuckGo search available")
except ImportError:
    print("⚠️ DuckDuckGo search not available")
    DDGS = None

# ─── System Info ───────────────────────────────────────────────────────────────
cpu_cores = psutil.cpu_count(logical=False)
available_ram_gb = psutil.virtual_memory().total / (1024**3)
print(f"💻 CPU: {cpu_cores} cores | RAM: {available_ram_gb:.1f} GB")

# ─── Discord Setup ──────────────────────────────────────────────────────────────
intents = discord.Intents.default()
intents.message_content = True
bot = discord.Client(intents=intents)
CHANNEL_ID = 1415387360509821018

# ─── Perplexity Config ─────────────────────────────────────────────────────────
PERPLEXITY_API_KEY = os.getenv("PERPLEXITY_API_KEY", "API KEY")
PERPLEXITY_URL = "https://api.perplexity.ai/chat/completions"

async def perplexity_check(query):
    headers = {
        "Authorization": f"Bearer {PERPLEXITY_API_KEY}",
        "Content-Type": "application/json"
    }
    payload = {
        "model": "sonar-pro",
        "messages": [{"role": "user", "content": query}],
        "max_tokens": 1000,
        "temperature": 0.2
    }
    loop = asyncio.get_event_loop()
    resp = await asyncio.wait_for(
        loop.run_in_executor(
            None,
            lambda: requests.post(PERPLEXITY_URL, headers=headers, json=payload, timeout=30)
        ),
        timeout=35.0
    )
    return (
        resp.json()["choices"][0]["message"]["content"]
        if resp.status_code == 200
        else f"API error {resp.status_code}"
    )

# ─── Response Sanitization ─────────────────────────────────────────────────────
def clean_llm_response(text: str) -> str:
    """Clean DeepSeek-R1 response artifacts and repetition"""
    if not text:
        return text
    
    # Remove thinking tags (common in DeepSeek-R1)
    text = re.sub(r'<\/?think>', '', text, flags=re.IGNORECASE)
    
    # Remove any XML-style reasoning tags
    text = re.sub(r'<\/[^>]*>', '', text)
    
    # Split into sentences and remove exact duplicates
    sentences = text.split('. ')
    cleaned_sentences = []
    seen_sentences = set()
    
    for sentence in sentences:
        sentence = sentence.strip()
        if sentence and sentence not in seen_sentences:
            cleaned_sentences.append(sentence)
            seen_sentences.add(sentence)
    
    # Rejoin and clean up
    cleaned = '. '.join(cleaned_sentences)
    
    # Remove duplicate consecutive phrases (more aggressive deduplication)
    words = cleaned.split()
    if len(words) > 10:  # Only for longer responses
        half = len(words) // 2
        first_half = ' '.join(words[:half])
        second_half = ' '.join(words[half:])
        if first_half.strip() == second_half.strip():
            cleaned = first_half.strip()
    
    return cleaned.strip()

# ─── GPU Memory Management ──────────────────────────────────────────────────────
def cleanup_gpu():
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
        gc.collect()

def get_gpu_status():
    if torch.cuda.is_available():
        total = torch.cuda.get_device_properties(0).total_memory / 1024**3
        allocated = torch.cuda.memory_allocated(0) / 1024**3
        return total, allocated, total - allocated
    return 0, 0, 0

# ─── Optimized Model Loading ────────────────────────────────────────────────────
MODEL_PATH = "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B"
print("🔄 Loading optimized DeepSeek-R1...")
gpu_memory_gb = (
    torch.cuda.get_device_properties(0).total_memory / 1024**3
    if torch.cuda.is_available()
    else 0
)
if gpu_memory_gb > 0:
    print(f"💾 GPU: {torch.cuda.get_device_name(0)} ({gpu_memory_gb:.1f} GB)")

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_quant_storage=torch.uint8,
)

cleanup_gpu()
print("📝 Loading tokenizer...")
tokenizer = AutoTokenizer.from_pretrained(
    MODEL_PATH, trust_remote_code=True, use_fast=True
)

print("🔄 Loading and compiling model...")
model = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH,
    quantization_config=quantization_config,
    device_map={"": 0},
    trust_remote_code=True,
    torch_dtype=torch.float16,
    max_memory={0: "11GB", "cpu": "20GB"}
)
model.eval()

try:
    compiled_model = torch.compile(model, mode="reduce-overhead")
    print("✅ Model compiled with torch.compile")
except Exception:
    compiled_model = model
    print("⚠️ torch.compile not available, using standard model")

print("🔥 Pre-warming model...")
dummy_input = tokenizer("Hello", return_tensors="pt", padding=True).to(compiled_model.device)
with torch.no_grad():
    compiled_model.generate(**dummy_input, max_new_tokens=5, do_sample=False)
del dummy_input
total_gpu, allocated_gpu, free_gpu = get_gpu_status()
print(f"✅ Model ready! GPU: {allocated_gpu:.1f}GB/{total_gpu:.1f}GB")
cleanup_gpu()

# ─── High-Performance Inference Wrapper ────────────────────────────────────────
class OptimizedDeepSeekR1:
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
        self.device = next(model.parameters()).device
        self.max_thinking = 3072  # Reduced to prevent over-generation
        self.max_fast = 1536     # Reduced to prevent over-generation
        self.system_msg = (
            "You are DeepSeek R1, an advanced AI assistant. "
            "Provide clear, concise responses without repetition."
        )
        print(f"🔧 Optimized LLM ready | Thinking: {self.max_thinking:,} | Fast: {self.max_fast:,}")

    def generate(self, messages, thinking=True):
        start_time = time.time()
        text = self.tokenizer.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )
        inputs = self.tokenizer(text, return_tensors="pt", max_length=32768, truncation=True)
        inputs = {k: v.to(self.device, non_blocking=True) for k, v in inputs.items()}

        # Improved generation parameters to reduce repetition
        gen_kwargs = {
            "max_new_tokens": self.max_thinking if thinking else self.max_fast,
            "do_sample": True,
            "temperature": 0.4 if thinking else 0.6,  # Reduced temperature
            "top_p": 0.9,  # More focused sampling
            "top_k": 40,
            "pad_token_id": self.tokenizer.eos_token_id,
            "eos_token_id": self.tokenizer.eos_token_id,
            "repetition_penalty": 1.1,  # Increased repetition penalty
            "use_cache": True,
            "early_stopping": True,  # Added early stopping
        }

        with torch.no_grad(), torch.amp.autocast("cuda", dtype=torch.float16):
            outputs = self.model.generate(**inputs, **gen_kwargs)

        new_tokens = outputs[0][len(inputs["input_ids"][0]) :]
        response = self.tokenizer.decode(new_tokens, skip_special_tokens=True).strip()
        
        # Clean response before returning
        response = clean_llm_response(response)
        
        elapsed = time.time() - start_time
        tok_per_sec = len(new_tokens) / elapsed if elapsed > 0 else 0

        del outputs, new_tokens, inputs
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        return response, tok_per_sec

# Initialize optimized LLM
llm = OptimizedDeepSeekR1(compiled_model, tokenizer)

# ─── OSINT Tools Manager (unchanged from your code) ─────────────────────────────
class OSINTToolsManager:
    def __init__(self, base_path: Path):
        self.base_path = base_path
        self.tools_path = base_path / "osint_tools"
        self.harvester_path = self.tools_path / "theHarvester"
        self.spiderfoot_path = self.tools_path / "spiderfoot"
        self.sherlock_path = self.tools_path / "sherlock"
        self.tools_path.mkdir(parents=True, exist_ok=True)

    def create_scan_record(self, target, tool):
        return int(time.time() * 1000)

    def update_scan_status(self, scan_id, status, count=0):
        pass

    def store_email_data(self, email, domain, tool, scan_id):
        pass

    def store_subdomain_data(self, subdomain, domain, tool, scan_id):
        pass

    def store_ip_data(self, ip, domain, tool, scan_id):
        pass

    def store_social_profile_data(self, username, platform, url, tool, scan_id):
        pass

    # [Include all your OSINT methods here - sherlock, harvester, spiderfoot]
    # ... (keeping the same implementation as your original code)

# Instantiate OSINT manager
osint_manager = OSINTToolsManager(Path.cwd())

# ─── Cached Search (unchanged) ──────────────────────────────────────────────────
web_cache = {}
CACHE_TTL = 86400

async def fast_ddg_search(query, max_results=5):
    if not DDGS:
        return ""
    cache_key = query.lower().strip()
    now = time.time()
    if cache_key in web_cache and now - web_cache[cache_key][0] < CACHE_TTL:
        return web_cache[cache_key][1]
    try:
        ddg = DDGS()
        results = [
            f"{i+1}. {r['title']}\n{r['body']}"
            for i, r in enumerate(ddg.text(query, max_results=max_results))
            if r.get("title") and r.get("body")
        ]
        text = "\n\n".join(results)
        web_cache[cache_key] = (now, text)
        if len(web_cache) > 8192:
            oldest = min(web_cache.keys(), key=lambda k: web_cache[k][0])
            del web_cache[oldest]
        return text
    except:
        return ""

# ─── Optimized Discord Helpers ──────────────────────────────────────────────────
async def fast_send(ch, txt):
    if not txt:
        await ch.send("No response.")
        return
    
    # Additional cleaning before sending
    txt = clean_llm_response(txt)
    
    chunks = []
    while txt:
        chunk = txt[:1900]
        if len(txt) > 1900:
            last_period = chunk.rfind(". ")
            if last_period > 950:
                chunk = chunk[: last_period + 1]
        chunks.append(chunk)
        txt = txt[len(chunk) :].strip()
    
    for i, chunk in enumerate(chunks):
        await ch.send(chunk)
        if i < len(chunks) - 1:
            await asyncio.sleep(0.2)

def quick_needs_search(q):
    return any(
        w in q.lower()
        for w in ["current", "latest", "news", "who", "what", "when", "where", "202"]
    )

# ─── Bot Events with Fixed Startup Message ──────────────────────────────────────
@bot.event
async def on_ready():
    print("✅ High-performance bot online:", bot.user)
    
    # Try to get the channel with error handling
    try:
        channel = bot.get_channel(CHANNEL_ID)
        if channel is None:
            print(f"❌ Could not find channel with ID {CHANNEL_ID}")
            print("Available channels:")
            for guild in bot.guilds:
                for ch in guild.text_channels:
                    print(f"  {ch.name}: {ch.id}")
            return
        
        total_gpu, allocated_gpu, free_gpu = get_gpu_status()

        # Fixed startup message
        startup_message = f"""🚀 **DeepSeek-R1 High-Performance Discord Bot Ready!**

**System Status:**
• GPU: {allocated_gpu:.1f}GB/{total_gpu:.1f}GB | Compiled: ✅
• Features: DuckDuckGo, Perplexity, OSINT Scans

**📋 Available Commands:**

**🧠 AI & Search Commands:**
• `!ask <question>` - Deep reasoning mode with thinking tokens
• `!fast <question>` - Quick response mode for simple queries  
• `!search <query>` - Web search + AI analysis
• `!check <query>` - Direct Perplexity API lookup

**🔍 OSINT Reconnaissance Tools:**

**🌾 TheHarvester - Email & Domain Intelligence**
• `!harvester <domain.com>` - Comprehensive domain reconnaissance
• **What it finds:** Email addresses, subdomains, IP addresses, hostnames
• **Sources:** Google, Bing, DuckDuckGo, LinkedIn, Twitter, and 40+ more
• **Usage Example:** `!harvester example.com`

**🕷️ SpiderFoot - Advanced OSINT Automation**  
• `!spiderfoot <domain.com>` - Multi-module intelligence gathering
• **What it finds:** DNS records, SSL certs, social media, vulnerabilities, leaked data
• **Modules:** 200+ reconnaissance modules for deep investigation
• **Usage Example:** `!spiderfoot target.org`

**🔍 Sherlock - Social Media Username Hunter**
• `!sherlock <username>` - Hunt usernames across 350+ social platforms
• **What it finds:** Active social media profiles, account existence verification
• **Platforms:** GitHub, Instagram, Twitter, Reddit, TikTok, LinkedIn, and 340+ more
• **Usage Example:** `!sherlock john_doe`
• **Output:** Direct profile URLs, response times, platform verification

**Ready for reconnaissance operations!** 🎯"""

        await channel.send(startup_message)
        print("✅ Startup message sent successfully")
        
    except Exception as e:
        print(f"❌ Error sending startup message: {e}")

@bot.event
async def on_message(message):
    if message.author == bot.user or message.channel.id != CHANNEL_ID:
        return

    content = message.content.strip()
    if content.startswith("!ask "):
        cmd, query = "ask", content[5:].strip()
    elif content.startswith("!fast "):
        cmd, query = "fast", content[6:].strip()
    elif content.startswith("!search "):
        cmd, query = "search", content[8:].strip()
    elif content.startswith("!check "):
        cmd, query = "check", content[7:].strip()
    elif content.startswith("!harvester "):
        cmd, query = "harvester", content[11:].strip()
    elif content.startswith("!spiderfoot "):
        cmd, query = "spiderfoot", content[12:].strip()
    elif content.startswith("!sherlock "):
        cmd, query = "sherlock", content[10:].strip()
    else:
        return

    if not query:
        await message.channel.send("Please provide a parameter.")
        return

    async with message.channel.typing():
        try:
            if cmd == "check":
                result = await perplexity_check(query)
                await fast_send(message.channel, f"🔍 **Perplexity:** {result}")
                return

            # [Include all your OSINT command handling here]
            # ... (same as your original code)

            # Build context for AI responses
            context = llm.system_msg
            if cmd in ("search", "ask") and quick_needs_search(query):
                web_results = await fast_ddg_search(query, 5)
                if web_results:
                    context += f"\n\nWeb results:\n{web_results}"

            messages = [
                {"role": "system", "content": context},
                {"role": "user", "content": query}
            ]
            thinking = (cmd == "ask")
            response, tok_per_sec = llm.generate(messages, thinking=thinking)
            tag = {
                "ask": "🧠",
                "fast": "⚡",
                "search": "🌐",
                "harvester": "🌾",
                "spiderfoot": "🕷️",
                "sherlock": "🔍",
            }.get(cmd, "🤖")
            await fast_send(message.channel, f"{tag} {response}")

        except Exception as e:
            await message.channel.send(f"❌ **Error:** {e}")

# ─── Launch Bot ──────────────────────────────────────────────────────────────────
if __name__ == "__main__":
    token = os.getenv("DISCORD_TOKEN")
    if not token:
        print("❌ Set DISCORD_TOKEN environment variable")
        exit(1)

    print("🚀 Launching high-performance DeepSeek-R1 bot...")
    try:
        bot.run(token, log_handler=None)
    except KeyboardInterrupt:
        print("👋 Shutting down...")
    except Exception as e:
        print(f"❌ Bot error: {e}")
    finally:
        cleanup_gpu()
        print("🧹 Cleanup completed")

