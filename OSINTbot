import os
import discord
import asyncio
import torch
import time
import requests
import gc
import psutil
import json
import re
import glob
import csv
from pathlib import Path
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    logging
)

# ─── Performance Optimizations ────────────────────────────────────────────────
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["MKL_NUM_THREADS"] = "8"
torch.set_num_threads(psutil.cpu_count(logical=False))
logging.set_verbosity_error()

print("🤖 DeepSeek-R1 High-Performance Discord Bot")
print("=" * 50)

# ─── System Info ───────────────────────────────────────────────────────────────
cpu_cores = psutil.cpu_count(logical=False)
available_ram_gb = psutil.virtual_memory().total / (1024**3)
print(f"💻 CPU: {cpu_cores} cores | RAM: {available_ram_gb:.1f} GB")

# ─── Discord Setup ──────────────────────────────────────────────────────────────
intents = discord.Intents.default()
intents.message_content = True
bot = discord.Client(intents=intents)
CHANNEL_ID = YOUR CHANNEL ID

# ─── Response Cleaning ──────────────────────────────────────────────────────────
def clean_llm_response(text: str) -> str:
    if not text:
        return text
    text = re.sub(r'</?think>', '', text, flags=re.IGNORECASE)
    text = re.sub(r'</[^>]*>', '', text)
    sentences = text.split('. ')
    cleaned, seen = [], set()
    for s in sentences:
        s = s.strip()
        if s and s not in seen:
            cleaned.append(s)
            seen.add(s)
    out = '. '.join(cleaned)
    words = out.split()
    if len(words) > 10:
        half = len(words) // 2
        if ' '.join(words[:half]).strip() == ' '.join(words[half:]).strip():
            out = ' '.join(words[:half]).strip()
    return out.strip()

# ─── Perplexity Config ─────────────────────────────────────────────────────────
PERPLEXITY_API_KEY = os.getenv("PERPLEXITY_API_KEY", "API KEY3")
PERPLEXITY_URL = "https://api.perplexity.ai/chat/completions"

async def perplexity_check(query: str) -> str:
    headers = {
        "Authorization": f"Bearer {PERPLEXITY_API_KEY}",
        "Content-Type": "application/json"
    }
    payload = {
        "model": "sonar-pro",
        "messages": [{"role": "user", "content": query}],
        "max_tokens": 1000,
        "temperature": 0.2
    }
    loop = asyncio.get_event_loop()
    resp = await asyncio.wait_for(
        loop.run_in_executor(None, lambda: requests.post(PERPLEXITY_URL, headers=headers, json=payload, timeout=30)),
        timeout=35.0
    )
    return resp.json()["choices"][0]["message"]["content"] if resp.status_code == 200 else f"API error {resp.status_code}"

# ─── GPU Memory Management ──────────────────────────────────────────────────────
def cleanup_gpu():
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
        gc.collect()

def get_gpu_status():
    if torch.cuda.is_available():
        total = torch.cuda.get_device_properties(0).total_memory / 1024**3
        alloc = torch.cuda.memory_allocated(0) / 1024**3
        return total, alloc, total - alloc
    return 0, 0, 0

# ─── Model Loading ─────────────────────────────────────────────────────────────
MODEL_PATH = "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B"
print("🔄 Loading optimized DeepSeek-R1...")
if torch.cuda.is_available():
    print(f"💾 GPU: {torch.cuda.get_device_name(0)} ({torch.cuda.get_device_properties(0).total_memory/1024**3:.1f} GB)")

quant_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_quant_storage=torch.uint8,
)
cleanup_gpu()
print("📝 Loading tokenizer...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True, use_fast=True)
print("🔄 Loading and compiling model...")
model = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH,
    quantization_config=quant_config,
    device_map={"": 0},
    trust_remote_code=True,
    torch_dtype=torch.float16,
    max_memory={0: "11GB", "cpu": "20GB"}
)
model.eval()
try:
    compiled_model = torch.compile(model, mode="reduce-overhead")
    print("✅ Model compiled")
except Exception:
    compiled_model = model
    print("⚠️ Compilation not available")
print("🔥 Pre-warming model…")
dummy = tokenizer("Hello", return_tensors="pt", padding=True).to(compiled_model.device)
with torch.no_grad():
    compiled_model.generate(**dummy, max_new_tokens=5, do_sample=False)
del dummy
total_gpu, alloc_gpu, _ = get_gpu_status()
print(f"✅ Model ready! GPU: {alloc_gpu:.1f}GB/{total_gpu:.1f}GB")
cleanup_gpu()

# ─── Inference Wrapper ─────────────────────────────────────────────────────────
class OptimizedDeepSeekR1:
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
        self.device = next(model.parameters()).device
        self.max_thinking = 3072
        self.max_fast = 1536
        self.system_msg = (
            "You are DeepSeek R1, an advanced AI assistant. "
            "Provide clear, concise responses without repetition."
        )
        print(f"🔧 LLM ready | Thinking: {self.max_thinking} | Fast: {self.max_fast}")

    def generate(self, messages, thinking=True):
        start = time.time()
        text = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        inputs = self.tokenizer(text, return_tensors="pt", max_length=32768, truncation=True)
        inputs = {k: v.to(self.device, non_blocking=True) for k, v in inputs.items()}
        gen_kwargs = {
            "max_new_tokens": self.max_thinking if thinking else self.max_fast,
            "do_sample": True,
            "temperature": 0.4 if thinking else 0.6,
            "top_p": 0.9,
            "top_k": 40,
            "pad_token_id": self.tokenizer.eos_token_id,
            "eos_token_id": self.tokenizer.eos_token_id,
            "repetition_penalty": 1.1,
            "use_cache": True,
            "early_stopping": True,
        }
        with torch.no_grad(), torch.amp.autocast("cuda", dtype=torch.float16):
            outputs = self.model.generate(**inputs, **gen_kwargs)
        new_tokens = outputs[0][len(inputs["input_ids"][0]):]
        response = self.tokenizer.decode(new_tokens, skip_special_tokens=True).strip()
        response = clean_llm_response(response)
        tokps = len(new_tokens) / max(1e-6, time.time() - start)
        del outputs, new_tokens, inputs
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        return response, tokps

llm = OptimizedDeepSeekR1(compiled_model, tokenizer)

# ─── OSINT Manager ─────────────────────────────────────────────────────────────
class OSINTToolsManager:
    def __init__(self, base: Path):
        self.base = base
        self.tools = base / "osint_tools"
        self.harv = self.tools / "theHarvester"
        self.sf = self.tools / "spiderfoot"
        self.sh = self.tools / "sherlock"
        self.tools.mkdir(exist_ok=True)
        self.spiderfoot_server = None
        self.sf_port = 5001

    def create_scan_record(self, *a): 
        return int(time.time() * 1000)
    
    def update_scan_status(self, *a): 
        pass
    
    def store_social_profile_data(self, *a): 
        pass

    async def ensure_spiderfoot_running(self):
        """Ensure SpiderFoot server is running"""
        try:
            # Check if server is already running
            resp = requests.get(f"http://127.0.0.1:{self.sf_port}", timeout=2)
            if resp.status_code == 200:
                return True
        except:
            pass

        # Start SpiderFoot server if not running
        if self.spiderfoot_server is None:
            sf_script = self.sf / "sf.py"
            if sf_script.exists():
                print(f"🕷️ Starting SpiderFoot server on port {self.sf_port}")
                self.spiderfoot_server = await asyncio.create_subprocess_exec(
                    "python3", str(sf_script), "-l", f"127.0.0.1:{self.sf_port}",
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE,
                    cwd=str(self.sf)
                )
                # Wait for server to start
                await asyncio.sleep(5)
                return True
        return False

    async def run_harvester_scan(self, domain, limit=100):
        scan_id = self.create_scan_record(domain, "harvester")
        repo = self.harv
        script = repo / "theHarvester.py"
        # Activate the repo venv automatically via "uv run"
        cmd = ["uv", "run", "theHarvester", "-d", domain, "-b", "all", "-l", str(limit)]
        print(f"🌾 Executing TheHarvester: {' '.join(cmd)}")
        proc = await asyncio.create_subprocess_exec(
            *cmd,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE,
            cwd=str(repo)
        )
        out, err = await proc.communicate()
        rc = proc.returncode
        text = out.decode(errors="ignore")
        if rc != 0:
            return {"scan_id": scan_id, "status": "failed", "error": err.decode(errors="ignore")}

        results = []
        for line in text.splitlines():
            m_email = re.search(r"Email:\s+(\S+@\S+)", line)
            m_sub = re.search(r"Hosts?\s+Found:\s+(\S+)", line)
            m_ip = re.search(r"(\d+\.\d+\.\d+\.\d+)", line)
            if m_email:
                results.append({"type":"email","value":m_email.group(1)})
            if m_sub:
                results.append({"type":"subdomain","value":m_sub.group(1)})
            if m_ip:
                results.append({"type":"ip","value":m_ip.group(1)})

        return {"scan_id": scan_id, "status": "completed", "results": results}

# In your on_message:

        if cmd == "harvester":
            await message.channel.send(f"🌾 Starting TheHarvester scan for `{query}`…")
            outcome = await osint_manager.run_harvester_scan(query)
            if outcome["status"] != "completed":
                return await fast_send(message.channel, f"🌾 Error: {outcome.get('error','unknown')}")
            items = outcome["results"]
            if not items:
                return await fast_send(message.channel, f"🌾 No assets found for `{query}`.")
            lines = [f"• **{r['type']}**: `{r['value']}`" for r in items]
            return await fast_send(message.channel, "🌾 TheHarvester results:\n" + "\n".join(lines))

    async def run_spiderfoot_scan(self, target, timeout=300):
        """Run SpiderFoot scan using CLI method"""
        try:
            scan_id = self.create_scan_record(target, "spiderfoot")
            
            # Method 1: Direct CLI scan (preferred)
            sf_script = self.sf / "sf.py"
            if sf_script.exists():
                cmd = [
                    "python3", str(sf_script),
                    "-s", target,
                    "-u", "all",  # Use all modules
                    "-o", "csv",  # Output as CSV
                    "-q"  # Quiet mode
                ]
                
                print(f"🕷️ Executing SpiderFoot: {' '.join(cmd)}")
                
                proc = await asyncio.create_subprocess_exec(
                    *cmd,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE,
                    cwd=str(self.sf)
                )
                
                stdout, stderr = await asyncio.wait_for(proc.communicate(), timeout=timeout)
                rc = proc.returncode
                
                s_out = stdout.decode(errors="ignore")
                s_err = stderr.decode(errors="ignore")
                
                print(f"[SpiderFoot rc={rc}]")
                if s_err:
                    print(f"SpiderFoot stderr: {s_err[:500]}")
                
                results = []
                
                # Parse CSV output from stdout
                if s_out and rc == 0:
                    lines = s_out.strip().split('\n')
                    for line in lines[1:]:  # Skip header
                        if line.strip():
                            try:
                                parts = line.split('\t')  # Tab-separated
                                if len(parts) >= 5:
                                    event_type = parts[2]
                                    data = parts[4]
                                    results.append({
                                        "type": event_type,
                                        "data": data,
                                        "target": target
                                    })
                            except:
                                continue
                
                if not results and rc != 0:
                    return {
                        "scan_id": scan_id,
                        "status": "failed", 
                        "error": s_err or "SpiderFoot scan failed"
                    }
                
                return {
                    "scan_id": scan_id,
                    "status": "completed",
                    "results": results[:50]  # Limit results
                }
            
            else:
                return {
                    "scan_id": scan_id,
                    "status": "failed",
                    "error": "SpiderFoot sf.py not found"
                }
                
        except asyncio.TimeoutError:
            return {
                "scan_id": scan_id,
                "status": "failed",
                "error": f"SpiderFoot scan timed out after {timeout}s"
            }
        except Exception as e:
            return {
                "scan_id": scan_id,
                "status": "failed",
                "error": str(e)
            }

    async def run_sherlock_scan(self, username, timeout=60):
        scan_id = self.create_scan_record(username, "sherlock")
        exe = self.sh / "sherlock" / "Scripts" / "sherlock.exe"
        cwd = str(self.sh / "sherlock")
        if exe.exists():
            cmd = [str(exe), username, "--print-found", "--timeout", str(timeout), "--csv"]
        else:
            cmd = ["python", "-m", "sherlock", username, "--print-found", "--timeout", str(timeout), "--csv"]
            cwd = str(self.sh)
        print("Exec:", *cmd)
        proc = await asyncio.create_subprocess_exec(
            *cmd,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE,
            cwd=cwd
        )
        out, err = await proc.communicate()
        rc = proc.returncode
        s_out = out.decode(errors="ignore")
        s_err = err.decode(errors="ignore")
        print(f"[Sherlock rc={rc}]")
        results = []
        for fpath in glob.glob(os.path.join(cwd, f"*{username}*.csv")):
            with open(fpath, newline="", encoding="utf-8", errors="ignore") as fh:
                reader = csv.reader(fh)
                for row in reader:
                    for col in row:
                        if col.startswith(("http://", "https://")):
                            results.append({"type": "social_profile", "url": col})
            try: os.remove(fpath)
            except: pass
        if not results:
            for line in s_out.splitlines():
                m = re.search(r"(https?://\S+)", line)
                if m:
                    results.append({"type": "social_profile", "url": m.group(1)})
        if rc != 0 and not results:
            return {"scan_id": scan_id, "status": "failed", "error": s_err or "Sherlock failed"}
        for r in results:
            self.store_social_profile_data(username, "", r["url"], "sherlock", scan_id)
        return {"scan_id": scan_id, "status": "completed", "results": results}

osint_manager = OSINTToolsManager(Path.cwd())

# ─── Helpers ───────────────────────────────────────────────────────────────────
async def fast_send(ch, txt):
    if not txt:
        return await ch.send("No response.")
    txt = clean_llm_response(txt)
    chunks = []
    while txt:
        c = txt[:1900]
        if len(txt) > 1900:
            i = c.rfind(". ")
            if i > 950:
                c = c[: i + 1]
        chunks.append(c)
        txt = txt[len(c):].strip()
    for idx, c in enumerate(chunks):
        await ch.send(c)
        if idx < len(chunks) - 1:
            await asyncio.sleep(0.2)

def quick_needs_search(q):
    return any(w in q.lower() for w in ["current", "latest", "news", "who", "what", "when", "where", "202"])

# ─── Bot Events ────────────────────────────────────────────────────────────────
@bot.event
async def on_ready():
    print("✅ Bot online:", bot.user)
    channel = bot.get_channel(CHANNEL_ID)
    if not channel:
        print(f"❌ Channel {CHANNEL_ID} not found.")
        return
    gpu, alloc, _ = get_gpu_status()
    startup_message = (
        "🚀 **DeepSeek-R1 High-Performance Discord Bot Ready!**\n\n"
        f"**System Status:**\n"
        f"• GPU: {alloc:.1f}GB/{gpu:.1f}GB | Compiled: ✅\n"
        f"• Features: Perplexity, OSINT Scans\n\n"
        "**📋 Available Commands:**\n\n"
        "**🧠 AI & Search Commands:**\n"
        "• `!ask <question>` - Deep reasoning mode with thinking tokens\n"
        "• `!fast <question>` - Quick response mode for simple queries\n"
        "• `!search <query>` - Web search + AI analysis\n"
        "• `!check <query>` - Direct Perplexity API lookup\n\n"
        "**🔍 OSINT Reconnaissance Tools:**\n\n"
        "**🌾 TheHarvester - Email & Domain Intelligence**\n"
        "• `!harvester <domain.com>` - Comprehensive domain reconnaissance\n"
        "• **What it finds:** Email addresses, subdomains, IP addresses, hostnames\n"
        "• **Sources:** Google, Bing, LinkedIn, Twitter, and 40+ more\n"
        "• **Usage Example:** `!harvester example.com`\n\n"
        "**🕷️ SpiderFoot - Advanced OSINT Automation**\n"
        "• `!spiderfoot <domain.com>` - Multi-module intelligence gathering\n"
        "• **What it finds:** DNS records, SSL certs, social media, vulnerabilities, leaked data\n"
        "• **Modules:** 200+ reconnaissance modules for deep investigation\n"
        "• **Usage Example:** `!spiderfoot target.org`\n\n"
        "**🔍 Sherlock - Social Media Username Hunter**\n"
        "• `!sherlock <username>` - Hunt usernames across 350+ social platforms\n"
        "• **What it finds:** Active social media profiles, account existence verification\n"
        "• **Platforms:** GitHub, Instagram, Twitter, Reddit, TikTok, LinkedIn, and 340+ more\n"
        "• **Usage Example:** `!sherlock john_doe`\n"
        "• **Output:** Direct profile URLs, response times, platform verification\n\n"
        "**📊 Results Format:**\n"
        "• Real-time scan status updates\n"
        "• Categorized findings (emails, subdomains, IPs, social profiles)\n"
        "• Clickable URLs for immediate profile access\n"
        "• Response time metrics for platform verification\n"
        "• Error reporting for troubleshooting\n\n"
        "**Ready for reconnaissance operations!** 🎯"
    )
    await channel.send(startup_message)

@bot.event
async def on_message(message):
    if message.author == bot.user or message.channel.id != CHANNEL_ID:
        return
    content = message.content.strip()
    cmd, query = None, None
    for key, prefix in [
        ("ask","!ask "),("fast","!fast "),("search","!search "),
        ("check","!check "),("harvester","!harvester "),
        ("spiderfoot","!spiderfoot "),("sherlock","!sherlock ")
    ]:
        if content.startswith(prefix):
            cmd, query = key, content[len(prefix):].strip()
            break
    if not cmd or not query:
        return

    async with message.channel.typing():
        try:
            if cmd == "check":
                res = await perplexity_check(query)
                return await fast_send(message.channel, f"🔍 {res}")

            # ─── SpiderFoot Handler ────────────────────────────────────────────
            if cmd == "spiderfoot":
                await message.channel.send(f"🕷️ Starting SpiderFoot scan for `{query}`…")
                out = await osint_manager.run_spiderfoot_scan(query)
                if out.get("status") != "completed":
                    return await fast_send(message.channel, f"🕷️ SpiderFoot Error: {out.get('error','unknown')}")
                
                results = out.get("results", [])
                if not results:
                    return await fast_send(message.channel, f"🕷️ No data found for `{query}`.")
                
                # Group results by type
                by_type = {}
                for r in results:
                    event_type = r.get("type", "unknown")
                    if event_type not in by_type:
                        by_type[event_type] = []
                    by_type[event_type].append(r.get("data", ""))
                
                # Format results
                summary_lines = [f"🕷️ **SpiderFoot Results for `{query}`:**"]
                summary_lines.append(f"**Total findings:** {len(results)}")
                summary_lines.append("")
                
                for event_type, data_list in sorted(by_type.items())[:10]:  # Limit to 10 types
                    clean_type = event_type.replace("_", " ").title()
                    summary_lines.append(f"**{clean_type}:** {len(data_list)} items")
                    for item in data_list[:3]:  # Show first 3 items
                        if len(item) < 100:
                            summary_lines.append(f"• {item}")
                    if len(data_list) > 3:
                        summary_lines.append(f"  *...and {len(data_list) - 3} more*")
                    summary_lines.append("")
                
                return await fast_send(message.channel, "\n".join(summary_lines))

            if cmd == "sherlock":
                await message.channel.send(f"🔍 Starting Sherlock for `{query}`…")
                out = await osint_manager.run_sherlock_scan(query)
                if out.get("status") != "completed":
                    return await fast_send(message.channel, f"🔍 Error: {out.get('error','unknown')}")
                urls = [r["url"] for r in out["results"]]
                if not urls:
                    return await fast_send(message.channel, f"🔍 No profiles for `{query}`.")
                lines = [f"• {u}" for u in urls]
                return await fast_send(message.channel, "🔍 Sherlock results:\n" + "\n".join(lines))

            # Add harvester handler here when ready...

            if cmd in ("ask","fast","search"):
                ctx = llm.system_msg
                msgs = [{"role":"system","content":ctx}, {"role":"user","content":query}]
                resp, _ = llm.generate(msgs, thinking=(cmd=="ask"))
                tag = {"ask":"🧠","fast":"⚡","search":"🌐"}.get(cmd,"🤖")
                return await fast_send(message.channel, f"{tag} {resp}")

        except Exception as e:
            await message.channel.send(f"❌ Error: {str(e)}")

if __name__ == "__main__":
    token = os.getenv("DISCORD_TOKEN")
    if not token:
        print("❌ Set DISCORD_TOKEN")
        exit(1)
    print("🚀 Launching bot...")
    try:
        bot.run(token)
    finally:
        cleanup_gpu()
