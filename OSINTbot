import os
import discord
import asyncio
import torch
import time
import requests
import gc
import psutil
import json
import re
import glob
import csv
from pathlib import Path
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    logging
)

# ─── Performance Optimizations ────────────────────────────────────────────────
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["MKL_NUM_THREADS"] = "8"
torch.set_num_threads(psutil.cpu_count(logical=False))
logging.set_verbosity_error()

print("🤖 DeepSeek-R1 High-Performance Discord Bot")
print("=" * 50)

# ─── System Info ───────────────────────────────────────────────────────────────
cpu_cores = psutil.cpu_count(logical=False)
available_ram_gb = psutil.virtual_memory().total / (1024**3)
print(f"💻 CPU: {cpu_cores} cores | RAM: {available_ram_gb:.1f} GB")

# ─── Discord Setup ──────────────────────────────────────────────────────────────
intents = discord.Intents.default()
intents.message_content = True
bot = discord.Client(intents=intents)
CHANNEL_ID = DISCORD CHANNEL ID

# Create downloads directory
downloads_dir = Path("downloads")
downloads_dir.mkdir(exist_ok=True)

# ─── Token Management Functions ─────────────────────────────────────────────────
def count_tokens(text: str, tokenizer) -> int:
    """Count tokens in text using the tokenizer"""
    try:
        tokens = tokenizer.encode(text, truncation=False, add_special_tokens=False)
        return len(tokens)
    except Exception as e:
        print(f"⚠️ Token counting error: {e}")
        # Fallback to rough character-based estimation (1 token ≈ 4 chars)
        return len(text) // 4

def parse_token_limit(query: str) -> tuple[str, int]:
    """Extract token limit from query and return cleaned query + token limit"""
    # Look for patterns like "tokens=1024" or "max_tokens=2048"
    token_pattern = r'\b(?:tokens?|max_tokens?)=(\d+)\b'
    match = re.search(token_pattern, query, re.IGNORECASE)
    
    if match:
        token_limit = int(match.group(1))
        # Remove the token specification from the query
        cleaned_query = re.sub(token_pattern, '', query, flags=re.IGNORECASE).strip()
        return cleaned_query, token_limit
    
    return query, 2048  # Default token limit

# ─── File Processing Functions ──────────────────────────────────────────────────
async def extract_text_from_attachment(attachment, token_limit: int = 2048):
    """Extract text content from Discord attachment with configurable token limits"""
    try:
        file_data = await attachment.read()
        filename = attachment.filename.lower()
        
        print(f"📎 Processing {filename} ({len(file_data)} bytes) with max {token_limit} tokens per chunk")
        
        # Handle different file types
        if filename.endswith(('.txt', '.md', '.py', '.js', '.html', '.css', '.xml', '.json')):
            text = file_data.decode('utf-8', errors='ignore')
            print(f"📎 Extracted {len(text)} characters from text file")
            return await process_large_text(text, filename, token_limit)
            
        elif filename.endswith('.csv'):
            content = file_data.decode('utf-8', errors='ignore')
            print(f"📎 Extracted {len(content)} characters from CSV")
            return await process_large_text(f"CSV Data:\n{content}", filename, token_limit)
            
        elif filename.endswith('.pdf'):
            print("📎 Attempting PDF processing...")
            try:
                import PyPDF2
                import io
                
                print("📎 PyPDF2 imported successfully")
                pdf_reader = PyPDF2.PdfReader(io.BytesIO(file_data))
                page_count = len(pdf_reader.pages)
                print(f"📎 PDF has {page_count} pages")
                
                # Extract all text first
                full_text = ""
                for i in range(page_count):
                    try:
                        page = pdf_reader.pages[i]
                        page_text = page.extract_text()
                        full_text += f"\n--- Page {i+1} ---\n{page_text}\n"
                        
                        if i % 10 == 0:  # Progress update every 10 pages
                            print(f"📎 Processed {i+1}/{page_count} pages")
                            
                    except Exception as page_error:
                        print(f"📎 Error on page {i+1}: {page_error}")
                        full_text += f"\n--- Page {i+1} (Error) ---\nCould not extract text from this page.\n"
                
                print(f"📎 Total PDF text extracted: {len(full_text)} characters from {page_count} pages")
                
                if full_text.strip():
                    return await process_large_text(full_text, filename, token_limit)
                else:
                    return "PDF processed but no readable text found. This might be a scanned document requiring OCR."
                    
            except ImportError as import_error:
                print(f"📎 PyPDF2 import failed: {import_error}")
                return f"PDF file detected but PyPDF2 not available. Error: {import_error}"
            except Exception as pdf_error:
                print(f"📎 PDF processing error: {pdf_error}")
                return f"Error reading PDF: {pdf_error}"
                
        elif filename.endswith(('.docx', '.doc')):
            try:
                import docx
                import io
                
                doc = docx.Document(io.BytesIO(file_data))
                text = ""
                for paragraph in doc.paragraphs:
                    text += paragraph.text + "\n"
                print(f"📎 Extracted {len(text)} characters from Word document")
                return await process_large_text(text, filename, token_limit)
            except ImportError:
                return "Word document detected but python-docx not installed."
            except Exception as e:
                return f"Error reading Word document: {str(e)}"
                
        elif filename.endswith(('.xlsx', '.xls')):
            try:
                import openpyxl
                import io
                
                workbook = openpyxl.load_workbook(io.BytesIO(file_data))
                text = ""
                for sheet_name in workbook.sheetnames:
                    sheet = workbook[sheet_name]
                    text += f"\nSheet: {sheet_name}\n"
                    for row in sheet.iter_rows(values_only=True):
                        text += "\t".join([str(cell) if cell is not None else "" for cell in row]) + "\n"
                print(f"📎 Extracted {len(text)} characters from Excel file")
                return await process_large_text(text, filename, token_limit)
            except ImportError:
                return "Excel file detected but openpyxl not installed."
            except Exception as e:
                return f"Error reading Excel file: {str(e)}"
                
        else:
            return f"Unsupported file type: {filename}\nSupported types: .txt, .md, .py, .js, .html, .css, .xml, .json, .csv, .pdf, .docx, .doc, .xlsx, .xls"
            
    except Exception as e:
        print(f"📎 General error processing {attachment.filename}: {e}")
        return f"Error processing attachment {attachment.filename}: {str(e)}"

async def process_large_text(full_text: str, filename: str, max_tokens: int = 2048) -> str:
    """Process large text in chunks limited by token count"""
    
    # Quick token estimate for the full text
    estimated_tokens = count_tokens(full_text, tokenizer)
    
    if estimated_tokens <= max_tokens:
        print(f"📎 Text is small enough ({estimated_tokens} tokens), processing normally")
        return full_text
    
    print(f"📎 Large text detected (~{estimated_tokens:,} tokens), processing in chunks of max {max_tokens:,} tokens")
    
    # Split text into token-limited chunks
    chunks = []
    start_idx = 0
    length = len(full_text)
    
    while start_idx < length:
        # Start with a conservative character estimate (1 token ≈ 4 characters)
        estimated_chars = max_tokens * 3
        end_idx = min(start_idx + estimated_chars, length)
        chunk_text = full_text[start_idx:end_idx]
        
        # Adjust chunk size to stay under token limit
        token_count = count_tokens(chunk_text, tokenizer)
        
        # If over limit, reduce chunk size
        while token_count > max_tokens and len(chunk_text) > 100:
            # Reduce by 10% each iteration
            chunk_text = chunk_text[:int(len(chunk_text) * 0.9)]
            token_count = count_tokens(chunk_text, tokenizer)
        
        # Try to break at sentence boundaries if possible
        if start_idx + len(chunk_text) < length:
            last_period = chunk_text.rfind('. ')
            last_newline = chunk_text.rfind('\n')
            break_point = max(last_period, last_newline)
            
            if break_point > len(chunk_text) * 0.7:  # Only break if we don't lose too much
                chunk_text = chunk_text[:break_point + 1]
        
        chunks.append(chunk_text)
        start_idx += len(chunk_text)
        
        # Prevent infinite loop
        if len(chunk_text) == 0:
            break
    
    total_chunks = len(chunks)
    print(f"📎 Split into {total_chunks} chunks for processing")
    
    # Process each chunk and get summaries
    chunk_summaries = []
    
    for i, chunk in enumerate(chunks):
        try:
            print(f"📎 Processing chunk {i+1}/{total_chunks} ({count_tokens(chunk, tokenizer)} tokens)")
            
            # Create a focused prompt for chunk analysis
            chunk_prompt = f"Analyze this section of the document '{filename}' (Part {i+1} of {total_chunks}) and provide key points, important information, and main concepts. Be concise but comprehensive:\n\n{chunk}"
            
            # Process this chunk with the AI
            messages = [
                {"role": "system", "content": "You are an AI assistant that analyzes document sections and extracts key information. Provide clear, organized summaries of the main points."},
                {"role": "user", "content": chunk_prompt}
            ]
            
            response, _ = llm.generate(messages, thinking=False)  # Use fast mode for chunks
            chunk_summaries.append(f"**Section {i+1}/{total_chunks}:**\n{response}")
            
            print(f"📎 Completed chunk {i+1}/{total_chunks}")
            
            # Small delay and GPU cleanup to prevent overwhelming
            await asyncio.sleep(0.5)
            cleanup_gpu()
            
        except Exception as chunk_error:
            print(f"📎 Error processing chunk {i+1}: {chunk_error}")
            chunk_summaries.append(f"**Section {i+1}/{total_chunks}:**\nError processing this section: {str(chunk_error)}")
    
    # Combine all summaries
    combined_summary = f"**COMPLETE ANALYSIS OF '{filename.upper()}'**\n\n"
    combined_summary += f"Document processed in {total_chunks} sections (~{estimated_tokens:,} total tokens, max {max_tokens} per chunk)\n\n"
    combined_summary += "\n\n".join(chunk_summaries)
    
    print(f"📎 Completed processing all {total_chunks} chunks")
    return combined_summary

# ─── Response Cleaning ──────────────────────────────────────────────────────────
def clean_llm_response(text: str) -> str:
    if not text:
        return text
    text = re.sub(r'</?think>', '', text, flags=re.IGNORECASE)
    text = re.sub(r'</[^>]*>', '', text)
    sentences = text.split('. ')
    cleaned, seen = [], set()
    for s in sentences:
        s = s.strip()
        if s and s not in seen:
            cleaned.append(s)
            seen.add(s)
    out = '. '.join(cleaned)
    words = out.split()
    if len(words) > 10:
        half = len(words) // 2
        if ' '.join(words[:half]).strip() == ' '.join(words[half:]).strip():
            out = ' '.join(words[:half]).strip()
    return out.strip()

# ─── Perplexity Config ─────────────────────────────────────────────────────────
PERPLEXITY_API_KEY = os.getenv("PERPLEXITY_API_KEY", "API KEY")
PERPLEXITY_URL = "https://api.perplexity.ai/chat/completions"

async def perplexity_check(query: str) -> str:
    headers = {
        "Authorization": f"Bearer {PERPLEXITY_API_KEY}",
        "Content-Type": "application/json"
    }
    payload = {
        "model": "sonar-pro",
        "messages": [{"role": "user", "content": query}],
        "max_tokens": 1000,
        "temperature": 0.2
    }
    loop = asyncio.get_event_loop()
    resp = await asyncio.wait_for(
        loop.run_in_executor(None, lambda: requests.post(PERPLEXITY_URL, headers=headers, json=payload, timeout=30)),
        timeout=35.0
    )
    return resp.json()["choices"][0]["message"]["content"] if resp.status_code == 200 else f"API error {resp.status_code}"

# ─── GPU Memory Management ──────────────────────────────────────────────────────
def cleanup_gpu():
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
        gc.collect()

def get_gpu_status():
    if torch.cuda.is_available():
        total = torch.cuda.get_device_properties(0).total_memory / 1024**3
        alloc = torch.cuda.memory_allocated(0) / 1024**3
        return total, alloc, total - alloc
    return 0, 0, 0

# ─── Model Loading ─────────────────────────────────────────────────────────────
MODEL_PATH = "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B"
print("🔄 Loading optimized DeepSeek-R1...")
if torch.cuda.is_available():
    print(f"💾 GPU: {torch.cuda.get_device_name(0)} ({torch.cuda.get_device_properties(0).total_memory/1024**3:.1f} GB)")

quant_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_quant_storage=torch.uint8,
)
cleanup_gpu()
print("📝 Loading tokenizer...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True, use_fast=True)
print("🔄 Loading and compiling model...")
model = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH,
    quantization_config=quant_config,
    device_map={"": 0},
    trust_remote_code=True,
    torch_dtype=torch.float16,
    max_memory={0: "11GB", "cpu": "20GB"}
)
model.eval()
try:
    compiled_model = torch.compile(model, mode="reduce-overhead")
    print("✅ Model compiled")
except Exception:
    compiled_model = model
    print("⚠️ Compilation not available")
print("🔥 Pre-warming model…")
dummy = tokenizer("Hello", return_tensors="pt", padding=True).to(compiled_model.device)
with torch.no_grad():
    compiled_model.generate(**dummy, max_new_tokens=5, do_sample=False)
del dummy
total_gpu, alloc_gpu, _ = get_gpu_status()
print(f"✅ Model ready! GPU: {alloc_gpu:.1f}GB/{total_gpu:.1f}GB")
cleanup_gpu()

# ─── Inference Wrapper ─────────────────────────────────────────────────────────
class OptimizedDeepSeekR1:
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
        self.device = next(model.parameters()).device
        self.max_thinking = 3072
        self.max_fast = 1536
        self.system_msg = (
            "You are DeepSeek R1, an advanced AI assistant. "
            "Provide clear, concise responses without repetition."
        )
        print(f"🔧 LLM ready | Thinking: {self.max_thinking} | Fast: {self.max_fast}")

    def generate(self, messages, thinking=True, max_tokens=None):
        start = time.time()
        text = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        inputs = self.tokenizer(text, return_tensors="pt", max_length=32768, truncation=True)
        inputs = {k: v.to(self.device, non_blocking=True) for k, v in inputs.items()}
        
        # Use custom max_tokens if provided, otherwise use defaults
        if max_tokens:
            max_new_tokens = min(max_tokens, self.max_thinking if thinking else self.max_fast)
        else:
            max_new_tokens = self.max_thinking if thinking else self.max_fast
            
        gen_kwargs = {
            "max_new_tokens": max_new_tokens,
            "do_sample": True,
            "temperature": 0.4 if thinking else 0.6,
            "top_p": 0.9,
            "top_k": 40,
            "pad_token_id": self.tokenizer.eos_token_id,
            "eos_token_id": self.tokenizer.eos_token_id,
            "repetition_penalty": 1.1,
            "use_cache": True,
            "early_stopping": True,
        }
        with torch.no_grad(), torch.amp.autocast("cuda", dtype=torch.float16):
            outputs = self.model.generate(**inputs, **gen_kwargs)
        new_tokens = outputs[0][len(inputs["input_ids"][0]):]
        response = self.tokenizer.decode(new_tokens, skip_special_tokens=True).strip()
        response = clean_llm_response(response)
        tokps = len(new_tokens) / max(1e-6, time.time() - start)
        del outputs, new_tokens, inputs
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        return response, tokps

llm = OptimizedDeepSeekR1(compiled_model, tokenizer)

# ─── OSINT Manager (unchanged) ─────────────────────────────────────────────────
class OSINTToolsManager:
    def __init__(self, base: Path):
        self.base = base
        self.tools = base / "osint_tools"
        self.harv = self.tools / "theHarvester"
        self.sf = self.tools / "spiderfoot"
        self.sh = self.tools / "sherlock"
        self.tools.mkdir(exist_ok=True)

    def create_scan_record(self, target, tool):
        return int(time.time() * 1000)

    def update_scan_status(self, *args):
        pass

    def store_social_profile_data(self, *args):
        pass

    async def run_harvester_scan(self, domain, limit=100):
        """Execute TheHarvester via 'uv run theHarvester -d <domain>'"""
        scan_id = self.create_scan_record(domain, "harvester")
        repo = self.harv
        cmd = ["uv", "run", "theHarvester", "-d", domain, "-l", str(limit)]
        print(f"🌾 Executing TheHarvester: {' '.join(cmd)}")
        
        try:
            proc = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
                cwd=str(repo)
            )
            out, err = await proc.communicate()
            rc = proc.returncode
            text = out.decode(errors="ignore")
            error = err.decode(errors="ignore")
            
            if rc != 0:
                return {"scan_id": scan_id, "status": "failed", "error": error}

            results = []
            lines = text.splitlines()
            for line in lines:
                if "@" in line and not line.startswith("["):
                    email_match = re.search(r"([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,})", line)
                    if email_match:
                        results.append({"type": "email", "value": email_match.group(1)})
                
                if domain in line and not line.startswith("[") and "." in line:
                    host_match = re.search(r"([a-zA-Z0-9.-]+\." + re.escape(domain.split('.')[-1]) + r")", line)
                    if host_match and host_match.group(1) != domain:
                        results.append({"type": "subdomain", "value": host_match.group(1)})
                
                ip_match = re.search(r"\b(\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})\b", line)
                if ip_match:
                    results.append({"type": "ip", "value": ip_match.group(1)})

            seen = set()
            unique_results = []
            for r in results:
                key = (r["type"], r["value"])
                if key not in seen:
                    seen.add(key)
                    unique_results.append(r)

            return {"scan_id": scan_id, "status": "completed", "results": unique_results}
        
        except Exception as e:
            return {"scan_id": scan_id, "status": "failed", "error": str(e)}

    # ... other OSINT methods remain unchanged ...

osint_manager = OSINTToolsManager(Path.cwd())

# ─── Helpers ───────────────────────────────────────────────────────────────────
async def fast_send(ch, txt):
    if not txt:
        return await ch.send("No response.")
    txt = clean_llm_response(txt)
    chunks = []
    while txt:
        c = txt[:1900]
        if len(txt) > 1900:
            i = c.rfind(". ")
            if i > 950:
                c = c[: i + 1]
        chunks.append(c)
        txt = txt[len(c):].strip()
    for idx, c in enumerate(chunks):
        await ch.send(c)
        if idx < len(chunks) - 1:
            await asyncio.sleep(0.2)

def quick_needs_search(q):
    return any(w in q.lower() for w in ["current", "latest", "news", "who", "what", "when", "where", "202"])

# ─── Bot Events ────────────────────────────────────────────────────────────────
@bot.event
async def on_ready():
    print("✅ Bot online:", bot.user)
    channel = bot.get_channel(CHANNEL_ID)
    if not channel:
        print(f"❌ Channel {CHANNEL_ID} not found.")
        return
    gpu, alloc, _ = get_gpu_status()
    
    startup_message_1 = (
        f"🚀 **DeepSeek-R1 High-Performance Discord Bot Ready!**\n\n"
        f"**System Status:**\n"
        f"• GPU: {alloc:.1f}GB/{gpu:.1f}GB | Compiled: ✅\n"
        f"• Features: Perplexity, OSINT Scans, File Analysis with Token Control\n\n"
        "**📋 Available Commands:**\n\n"
        "**🧠 AI & Search Commands:**\n"
        "• `!ask <question>` - Deep reasoning mode (supports file attachments)\n"
        "• `!ask tokens=1024 <question>` - Set custom token limit per chunk\n"
        "• `!fast <question>` - Quick response mode\n"
        "• `!search <query>` - Web search + AI analysis\n"
        "• `!check <query>` - Direct Perplexity API lookup\n\n"
        "**📎 File Analysis with Token Control:**\n"
        "• Attach files to `!ask` commands for analysis\n"
        "• **Supported:** .txt, .md, .py, .js, .html, .css, .xml, .json, .csv, .pdf, .docx, .doc, .xlsx, .xls\n"
        "• **Usage:** `!ask tokens=512 read this book and give me key points`\n"
        "• **Token limits:** 256-4096 tokens per chunk (default: 2048)"
    )
    
    startup_message_2 = (
        "**🔍 OSINT Reconnaissance Tools:** (unchanged)\n\n"
        "**🌾 TheHarvester** • `!harvester <domain.com>`\n"
        "**🕷️ SpiderFoot** • `!spiderfoot <domain.com>`\n"
        "**🔍 Sherlock** • `!sherlock <username>`\n\n"
        "**📊 Token Control Examples:**\n"
        "• `!ask tokens=512 analyze this PDF` - Small chunks, fast processing\n"
        "• `!ask tokens=2048 summarize this book` - Balanced chunks (default)\n"
        "• `!ask tokens=4096 detailed analysis of document` - Large chunks, comprehensive\n\n"
        "**Ready for reconnaissance and document analysis!** 🎯"
    )
    
    try:
        await channel.send(startup_message_1)
        await asyncio.sleep(1)
        await channel.send(startup_message_2)
        print("✅ Startup messages sent successfully to Discord!")
    except Exception as e:
        print(f"❌ Error sending startup messages: {e}")

@bot.event
async def on_message(message):
    if message.author == bot.user or message.channel.id != CHANNEL_ID:
        return
    
    content = message.content.strip()
    cmd, query = None, None
    for key, prefix in [
        ("ask", "!ask "), ("fast", "!fast "), ("search", "!search "),
        ("check", "!check "), ("harvester", "!harvester "),
        ("spiderfoot", "!spiderfoot "), ("sherlock", "!sherlock ")
    ]:
        if content.startswith(prefix):
            cmd, query = key, content[len(prefix):].strip()
            break
    
    if not cmd or not query:
        return

    async with message.channel.typing():
        try:
            if cmd == "check":
                res = await perplexity_check(query)
                return await fast_send(message.channel, f"🔍 {res}")

            # ─── Enhanced !ask Handler with Token Control ──────────────────────
            if cmd in ("ask", "fast", "search"):
                # Parse token limit from query
                if cmd == "ask":
                    query, token_limit = parse_token_limit(query)
                    print(f"📎 Using token limit: {token_limit}")
                else:
                    token_limit = 2048  # Default for non-ask commands
                
                # Validate token limit
                if token_limit < 256:
                    token_limit = 256
                elif token_limit > 4096:
                    token_limit = 4096
                
                # Check for file attachments
                file_content = ""
                if message.attachments and cmd == "ask":
                    await message.channel.send(f"📎 Processing file attachments with {token_limit} tokens per chunk...")
                    
                    for attachment in message.attachments:
                        if attachment.size > 50 * 1024 * 1024:  # 50MB limit
                            await message.channel.send(f"⚠️ File `{attachment.filename}` is too large (max 50MB)")
                            continue
                            
                        print(f"📎 Processing attachment: {attachment.filename} ({attachment.size} bytes)")
                        
                        extracted_text = await extract_text_from_attachment(attachment, token_limit)
                        if extracted_text:
                            # For large documents, the extracted_text is already the complete analysis
                            if len(extracted_text) > 20000 and "COMPLETE ANALYSIS" in extracted_text:
                                # This is a pre-processed large document analysis
                                tag = f"🧠📎 (tokens={token_limit})"
                                return await fast_send(message.channel, f"{tag} **Large Document Analysis Complete**\n\n{extracted_text}")
                            else:
                                # Normal file processing
                                file_content += f"\n\n--- Content from {attachment.filename} ---\n{extracted_text}\n"
                
                # Build context for AI responses
                ctx = llm.system_msg
                if file_content:
                    ctx += f"\n\nThe user has provided the following file content for analysis:\n{file_content}"
                    query = f"{query}\n\nPlease analyze the provided file content and answer the question accordingly."
                
                msgs = [{"role": "system", "content": ctx}, {"role": "user", "content": query}]
                resp, _ = llm.generate(msgs, thinking=(cmd == "ask"))
                tag = {"ask": "🧠", "fast": "⚡", "search": "🌐"}.get(cmd, "🤖")
                
                if file_content:
                    file_info = f" 📎 (analyzed {len(message.attachments)} file{'s' if len(message.attachments) > 1 else ''})"
                    if cmd == "ask":
                        file_info += f" tokens={token_limit}"
                    tag += file_info
                
                return await fast_send(message.channel, f"{tag} {resp}")

            # ─── OSINT Handlers (unchanged) ──────────────────────────────────────
            # ... harvester, spiderfoot, sherlock handlers remain the same ...

        except Exception as e:
            await message.channel.send(f"❌ Error: {str(e)}")

if __name__ == "__main__":
    token = os.getenv("DISCORD_TOKEN")
    if not token:
        print("❌ Set DISCORD_TOKEN")
        exit(1)
    print("🚀 Launching bot...")
    try:
        bot.run(token)
    finally:
        cleanup_gpu()
