import os
import discord
import asyncio
import torch
import time
import requests
import gc
import psutil
import json
import re
import glob
import csv
from pathlib import Path
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    logging
)

# ─── Performance Optimizations ────────────────────────────────────────────────
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["MKL_NUM_THREADS"] = "8"
torch.set_num_threads(psutil.cpu_count(logical=False))
logging.set_verbosity_error()

print("🤖 DeepSeek-R1 High-Performance Discord Bot")
print("=" * 50)

# ─── System Info ───────────────────────────────────────────────────────────────
cpu_cores = psutil.cpu_count(logical=False)
available_ram_gb = psutil.virtual_memory().total / (1024**3)
print(f"💻 CPU: {cpu_cores} cores | RAM: {available_ram_gb:.1f} GB")

# ─── Discord Setup ──────────────────────────────────────────────────────────────
intents = discord.Intents.default()
intents.message_content = True
bot = discord.Client(intents=intents)
CHANNEL_ID = CHANNEL ID FOR DISCORD

# Create downloads directory
downloads_dir = Path("downloads")
downloads_dir.mkdir(exist_ok=True)

# ─── Utility: token counting and limits ─────────────────────────────────────────
def count_tokens(text: str, tokenizer) -> int:
    try:
        toks = tokenizer.encode(text, truncation=False, add_special_tokens=False)
        return len(toks)
    except Exception as e:
        print(f"⚠️ Token count fallback due to: {e}")
        return max(1, len(text) // 4)

def parse_token_limit(query: str, default_tokens: int = 2048) -> tuple[str, int]:
    # Accept tokens=1024 or max_tokens=1024 in the query
    m = re.search(r'\b(?:tokens|max_tokens)=(\d+)\b', query, flags=re.IGNORECASE)
    if not m:
        return query, default_tokens
    limit = int(m.group(1))
    limit = max(256, min(4096, limit))  # clamp to safe range
    cleaned = re.sub(r'\b(?:tokens|max_tokens)=(\d+)\b', '', query, flags=re.IGNORECASE).strip()
    return cleaned, limit

# ─── File Processing Functions ──────────────────────────────────────────────────
async def extract_text_from_attachment(attachment, token_limit: int = 2048):
    """Extract text content from Discord attachment and return a combined analysis for big files"""
    try:
        file_data = await attachment.read()
        filename = attachment.filename.lower()
        print(f"📎 Processing {filename} ({len(file_data)} bytes) with max {token_limit} tokens/chunk")

        # Plain text-like files
        if filename.endswith(('.txt', '.md', '.py', '.js', '.html', '.css', '.xml', '.json')):
            text = file_data.decode('utf-8', errors='ignore')
            print(f"📎 Extracted {len(text)} characters from text file")
            return await process_large_text(text, filename, token_limit)

        # CSV
        if filename.endswith('.csv'):
            content = file_data.decode('utf-8', errors='ignore')
            return await process_large_text(f"CSV Data:\n{content}", filename, token_limit)

        # PDF
        if filename.endswith('.pdf'):
            try:
                import PyPDF2
                import io
                pdf_reader = PyPDF2.PdfReader(io.BytesIO(file_data))
                page_count = len(pdf_reader.pages)
                print(f"📎 PDF pages: {page_count}")
                full_text = ""
                for i in range(page_count):
                    try:
                        page = pdf_reader.pages[i]
                        page_text = page.extract_text() or ""
                        full_text += f"\n--- Page {i+1} ---\n{page_text}\n"
                        if (i + 1) % 10 == 0:
                            print(f"📎 Processed {i+1}/{page_count} pages")
                    except Exception as pe:
                        print(f"📎 Page {i+1} error: {pe}")
                        full_text += f"\n--- Page {i+1} (Error) ---\n<no text>\n"
                if full_text.strip():
                    return await process_large_text(full_text, filename, token_limit)
                return "PDF processed but no readable text found. It may be a scanned document that needs OCR."
            except ImportError as ie:
                return f"PDF detected but PyPDF2 not installed: {ie}"
            except Exception as e:
                return f"Error reading PDF: {e}"

        # Word
        if filename.endswith(('.docx', '.doc')):
            try:
                import docx
                import io
                doc = docx.Document(io.BytesIO(file_data))
                text = "\n".join(p.text for p in doc.paragraphs)
                return await process_large_text(text, filename, token_limit)
            except ImportError:
                return "Word document detected but python-docx not installed."
            except Exception as e:
                return f"Error reading Word document: {e}"

        # Excel
        if filename.endswith(('.xlsx', '.xls')):
            try:
                import openpyxl
                import io
                wb = openpyxl.load_workbook(io.BytesIO(file_data))
                text = ""
                for sn in wb.sheetnames:
                    sh = wb[sn]
                    text += f"\nSheet: {sn}\n"
                    for row in sh.iter_rows(values_only=True):
                        text += "\t".join("" if c is None else str(c) for c in row) + "\n"
                return await process_large_text(text, filename, token_limit)
            except ImportError:
                return "Excel file detected but openpyxl not installed."
            except Exception as e:
                return f"Error reading Excel file: {e}"

        return f"Unsupported file type: {filename}"

    except Exception as e:
        print(f"📎 General error processing {attachment.filename}: {e}")
        return f"Error processing attachment {attachment.filename}: {e}"

async def process_large_text(full_text: str, filename: str, max_tokens: int = 2048) -> str:
    """Split by token budget and summarize each chunk, then combine"""
    total_est_toks = count_tokens(full_text, tokenizer)
    if total_est_toks <= max_tokens:
        return full_text

    print(f"📎 Large text (~{total_est_toks} toks), chunking at {max_tokens} toks")
    chunks = []
    start = 0
    n = len(full_text)

    # Use a conservative char window; adjust to fit token limit
    while start < n:
        window = max_tokens * 3
        end = min(n, start + window)
        piece = full_text[start:end]
        # shrink until within token budget
        while count_tokens(piece, tokenizer) > max_tokens and len(piece) > 200:
            piece = piece[:-200]
        # try to break at a boundary
        if end < n:
            bp = max(piece.rfind('. '), piece.rfind('\n'))
            if bp > len(piece) * 0.6:
                piece = piece[:bp+1]
        if not piece:
            break
        chunks.append(piece)
        start += len(piece)

    print(f"📎 Built {len(chunks)} chunks")

    # Summarize each chunk quickly
    summaries = []
    for idx, ch in enumerate(chunks, 1):
        try:
            prompt = (
                f"Analyze this section of '{filename}' (Part {idx} of {len(chunks)}) and list key points, "
                f"important details, and main concepts, concisely:\n\n{ch}"
            )
            msgs = [
                {"role": "system", "content": "Extract clear, organized key points from the provided section."},
                {"role": "user", "content": prompt}
            ]
            resp, _ = llm.generate(msgs, thinking=False)
            summaries.append(f"Section {idx}/{len(chunks)}:\n{resp}")
            await asyncio.sleep(0.25)
            cleanup_gpu()
        except Exception as e:
            summaries.append(f"Section {idx}/{len(chunks)}: Error: {e}")

    combined = (
        f"**COMPLETE ANALYSIS OF '{filename.upper()}'**\n\n"
        f"Processed in {len(chunks)} sections (~{total_est_toks} tokens total, ~{max_tokens} tokens per chunk)\n\n"
        + "\n\n".join(summaries)
    )
    return combined

def truncate_text(text: str, filename: str, max_chars: int = 20000) -> str:
    if len(text) <= max_chars:
        return text
    truncated = text[:max_chars]
    last_period = truncated.rfind('. ')
    if last_period > max_chars * 0.8:
        truncated = truncated[:last_period + 1]
    total_chars = len(text)
    truncated += (
        f"\n\n[NOTE: File '{filename}' truncated from {total_chars:,} to {len(truncated):,} characters.]"
    )
    print(f"📎 Truncated {filename} to {len(truncated)} chars")
    return truncated

# ─── Cleaning ──────────────────────────────────────────────────────────────────
def clean_llm_response(text: str) -> str:
    if not text:
        return text
    text = re.sub(r'</?think>', '', text, flags=re.IGNORECASE)
    text = re.sub(r'</[^>]*>', '', text)
    sentences = text.split('. ')
    cleaned, seen = [], set()
    for s in sentences:
        s = s.strip()
        if s and s not in seen:
            cleaned.append(s)
            seen.add(s)
    out = '. '.join(cleaned)
    words = out.split()
    if len(words) > 10:
        half = len(words) // 2
        if ' '.join(words[:half]).strip() == ' '.join(words[half:]).strip():
            out = ' '.join(words[:half]).strip()
    return out.strip()

# ─── Perplexity Config ─────────────────────────────────────────────────────────
PERPLEXITY_API_KEY = os.getenv("PERPLEXITY_API_KEY", "API KEY")
PERPLEXITY_URL = "https://api.perplexity.ai/chat/completions"

async def perplexity_check(query: str) -> str:
    headers = {"Authorization": f"Bearer {PERPLEXITY_API_KEY}", "Content-Type": "application/json"}
    payload = {
        "model": "sonar-pro",
        "messages": [{"role": "user", "content": query}],
        "max_tokens": 1000,
        "temperature": 0.2
    }
    loop = asyncio.get_event_loop()
    resp = await asyncio.wait_for(
        loop.run_in_executor(None, lambda: requests.post(PERPLEXITY_URL, headers=headers, json=payload, timeout=30)),
        timeout=35.0
    )
    return resp.json()["choices"]["message"]["content"] if resp.status_code == 200 else f"API error {resp.status_code}"

# ─── GPU Memory Management ──────────────────────────────────────────────────────
def cleanup_gpu():
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
        gc.collect()

def get_gpu_status():
    if torch.cuda.is_available():
        total = torch.cuda.get_device_properties(0).total_memory / 1024**3
        alloc = torch.cuda.memory_allocated(0) / 1024**3
        return total, alloc, total - alloc
    return 0, 0, 0

# ─── Model Loading ─────────────────────────────────────────────────────────────
MODEL_PATH = "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B"
print("🔄 Loading optimized DeepSeek-R1...")
if torch.cuda.is_available():
    print(f"💾 GPU: {torch.cuda.get_device_name(0)} ({torch.cuda.get_device_properties(0).total_memory/1024**3:.1f} GB)")

quant_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_quant_storage=torch.uint8,
)
cleanup_gpu()
print("📝 Loading tokenizer...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True, use_fast=True)
print("🔄 Loading and compiling model...")
model = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH,
    quantization_config=quant_config,
    device_map={"": 0},
    trust_remote_code=True,
    torch_dtype=torch.float16,
    max_memory={0: "11GB", "cpu": "20GB"}
)
model.eval()
try:
    compiled_model = torch.compile(model, mode="reduce-overhead")
    print("✅ Model compiled")
except Exception:
    compiled_model = model
    print("⚠️ Compilation not available")
print("🔥 Pre-warming model…")
dummy = tokenizer("Hello", return_tensors="pt", padding=True).to(compiled_model.device)
with torch.no_grad():
    compiled_model.generate(**dummy, max_new_tokens=5, do_sample=False)
del dummy
total_gpu, alloc_gpu, _ = get_gpu_status()
print(f"✅ Model ready! GPU: {alloc_gpu:.1f}GB/{total_gpu:.1f}GB")
cleanup_gpu()

# ─── Inference Wrapper ─────────────────────────────────────────────────────────
class OptimizedDeepSeekR1:
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
        self.device = next(model.parameters()).device
        self.max_thinking = 3072
        self.max_fast = 1536
        self.system_msg = (
            "You are DeepSeek R1, an advanced AI assistant. "
            "Provide clear, concise responses without repetition."
        )
        print(f"🔧 LLM ready | Thinking: {self.max_thinking} | Fast: {self.max_fast}")

    def generate(self, messages, thinking=True):
        start = time.time()
        text = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        inputs = self.tokenizer(text, return_tensors="pt", max_length=32768, truncation=True)
        inputs = {k: v.to(self.device, non_blocking=True) for k, v in inputs.items()}
        gen_kwargs = {
            "max_new_tokens": self.max_thinking if thinking else self.max_fast,
            "do_sample": True,
            "temperature": 0.4 if thinking else 0.6,
            "top_p": 0.9,
            "top_k": 40,
            "pad_token_id": self.tokenizer.eos_token_id,
            "eos_token_id": self.tokenizer.eos_token_id,
            "repetition_penalty": 1.1,
            "use_cache": True,
            "early_stopping": True,
        }
        with torch.no_grad(), torch.amp.autocast("cuda", dtype=torch.float16):
            outputs = self.model.generate(**inputs, **gen_kwargs)
        new_tokens = outputs[len(inputs["input_ids"]):]
        response = self.tokenizer.decode(new_tokens, skip_special_tokens=True).strip()
        response = clean_llm_response(response)
        tokps = len(new_tokens) / max(1e-6, time.time() - start)
        del outputs, new_tokens, inputs
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        return response, tokps

llm = OptimizedDeepSeekR1(compiled_model, tokenizer)

# ─── OSINT Manager (unchanged logic) ───────────────────────────────────────────
class OSINTToolsManager:
    def __init__(self, base: Path):
        self.base = base
        self.tools = base / "osint_tools"
        self.harv = self.tools / "theHarvester"
        self.sf = self.tools / "spiderfoot"
        self.sh = self.tools / "sherlock"
        self.tools.mkdir(exist_ok=True)

    def create_scan_record(self, target, tool):
        return int(time.time() * 1000)

    def update_scan_status(self, *args):
        pass

    def store_social_profile_data(self, *args):
        pass

    async def run_harvester_scan(self, domain, limit=100):
        scan_id = self.create_scan_record(domain, "harvester")
        repo = self.harv
        cmd = ["uv", "run", "theHarvester", "-d", domain, "-l", str(limit)]
        print(f"🌾 Executing TheHarvester: {' '.join(cmd)}")
        try:
            proc = await asyncio.create_subprocess_exec(
                *cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE, cwd=str(repo)
            )
            out, err = await proc.communicate()
            rc = proc.returncode
            text = out.decode(errors="ignore")
            error = err.decode(errors="ignore")
            if rc != 0:
                return {"scan_id": scan_id, "status": "failed", "error": error}
            results = []
            for line in text.splitlines():
                if "@" in line and not line.startswith("["):
                    m = re.search(r"([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,})", line)
                    if m:
                        results.append({"type": "email", "value": m.group(1)})
                if domain in line and not line.startswith("[") and "." in line:
                    m = re.search(r"([a-zA-Z0-9.-]+\." + re.escape(domain.split('.')[-1]) + r")", line)
                    if m and m.group(1) != domain:
                        results.append({"type": "subdomain", "value": m.group(1)})
                m = re.search(r"\b(\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})\b", line)
                if m:
                    results.append({"type": "ip", "value": m.group(1)})
            seen = set()
            uniq = []
            for r in results:
                k = (r["type"], r["value"])
                if k not in seen:
                    seen.add(k)
                    uniq.append(r)
            return {"scan_id": scan_id, "status": "completed", "results": uniq}
        except Exception as e:
            return {"scan_id": scan_id, "status": "failed", "error": str(e)}

    async def run_spiderfoot_scan(self, target, timeout=300):
        try:
            scan_id = self.create_scan_record(target, "spiderfoot")
            sf_script = self.sf / "sf.py"
            if sf_script.exists():
                cmd = ["python3", str(sf_script), "-s", target, "-u", "all", "-o", "csv", "-q"]
                proc = await asyncio.create_subprocess_exec(
                    *cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE, cwd=str(self.sf)
                )
                stdout, stderr = await asyncio.wait_for(proc.communicate(), timeout=timeout)
                rc = proc.returncode
                s_out = stdout.decode(errors="ignore")
                s_err = stderr.decode(errors="ignore")
                results = []
                if s_out and rc == 0:
                    lines = s_out.strip().split('\n')
                    for line in lines[1:]:
                        if line.strip():
                            try:
                                parts = line.split('\t')
                                if len(parts) >= 5:
                                    results.append({"type": parts[2], "data": parts[4], "target": target})
                            except:
                                continue
                if not results and rc != 0:
                    return {"scan_id": scan_id, "status": "failed", "error": s_err or "SpiderFoot scan failed"}
                return {"scan_id": scan_id, "status": "completed", "results": results[:50]}
            else:
                return {"scan_id": scan_id, "status": "failed", "error": "SpiderFoot sf.py not found"}
        except asyncio.TimeoutError:
            return {"scan_id": scan_id, "status": "failed", "error": f"SpiderFoot scan timed out after {timeout}s"}
        except Exception as e:
            return {"scan_id": scan_id, "status": "failed", "error": str(e)}

    async def run_sherlock_scan(self, username, timeout=60):
        scan_id = self.create_scan_record(username, "sherlock")
        exe = self.sh / "sherlock" / "Scripts" / "sherlock.exe"
        cwd = str(self.sh / "sherlock") if exe.exists() else str(self.sh)
        cmd = [str(exe), username, "--print-found", "--timeout", str(timeout), "--csv"] if exe.exists() else \
              ["python", "-m", "sherlock", username, "--print-found", "--timeout", str(timeout), "--csv"]
        print("Exec:", *cmd)
        proc = await asyncio.create_subprocess_exec(
            *cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE, cwd=cwd
        )
        out, err = await proc.communicate()
        rc = proc.returncode
        s_out = out.decode(errors="ignore")
        s_err = err.decode(errors="ignore")
        results = []
        for fpath in glob.glob(os.path.join(cwd, f"*{username}*.csv")):
            with open(fpath, newline="", encoding="utf-8", errors="ignore") as fh:
                reader = csv.reader(fh)
                for row in reader:
                    for col in row:
                        if col.startswith(("http://", "https://")):
                            results.append({"type": "social_profile", "url": col})
            try:
                os.remove(fpath)
            except:
                pass
        if not results:
            for line in s_out.splitlines():
                m = re.search(r"(https?://\S+)", line)
                if m:
                    results.append({"type": "social_profile", "url": m.group(1)})
        if rc != 0 and not results:
            return {"scan_id": scan_id, "status": "failed", "error": s_err or "Sherlock failed"}
        return {"scan_id": scan_id, "status": "completed", "results": results}

osint_manager = OSINTToolsManager(Path.cwd())

# ─── Helpers ───────────────────────────────────────────────────────────────────
async def fast_send(ch, txt):
    if not txt:
        return await ch.send("No response.")
    txt = clean_llm_response(txt)
    chunks = []
    while txt:
        c = txt[:1900]
        if len(txt) > 1900:
            i = c.rfind(". ")
            if i > 950:
                c = c[: i + 1]
        chunks.append(c)
        txt = txt[len(c):].strip()
    for idx, c in enumerate(chunks):
        await ch.send(c)
        if idx < len(chunks) - 1:
            await asyncio.sleep(0.2)

def quick_needs_search(q):
    return any(w in q.lower() for w in ["current", "latest", "news", "who", "what", "when", "where", "202"])

# ─── Bot Events ────────────────────────────────────────────────────────────────
@bot.event
async def on_ready():
    print("✅ Bot online:", bot.user)
    channel = bot.get_channel(CHANNEL_ID)
    if not channel:
        print(f"❌ Channel {CHANNEL_ID} not found.")
        return
    gpu, alloc, _ = get_gpu_status()
    startup_message_1 = (
        f"🚀 **DeepSeek-R1 High-Performance Discord Bot Ready!**\n\n"
        f"**System Status:**\n"
        f"• GPU: {alloc:.1f}GB/{gpu:.1f}GB | Compiled: ✅\n"
        f"• Features: Perplexity, OSINT Scans, File Analysis\n\n"
        "**📋 Available Commands:**\n\n"
        "**🧠 AI & Search Commands:**\n"
        "• `!ask <question>` (supports file attachments; add `tokens=1024` to set chunk size)\n"
        "• `!fast <question>`\n"
        "• `!search <query>`\n"
        "• `!check <query>`"
    )
    startup_message_2 = (
        "**🔍 OSINT Tools:**\n"
        "• `!harvester <domain.com>`\n"
        "• `!spiderfoot <domain.com>`\n"
        "• `!sherlock <username>`\n\n"
        "**📎 Supported files:** txt, md, py, js, html, css, xml, json, csv, pdf, docx, xlsx"
    )
    try:
        await channel.send(startup_message_1)
        await asyncio.sleep(1)
        await channel.send(startup_message_2)
    except Exception as e:
        print(f"❌ Error sending startup messages: {e}")

@bot.event
async def on_message(message):
    if message.author == bot.user or message.channel.id != CHANNEL_ID:
        return

    content = message.content.strip()
    cmd, query = None, None
    for key, prefix in [
        ("ask", "!ask "), ("fast", "!fast "), ("search", "!search "),
        ("check", "!check "), ("harvester", "!harvester "),
        ("spiderfoot", "!spiderfoot "), ("sherlock", "!sherlock ")
    ]:
        if content.startswith(prefix):
            cmd, query = key, content[len(prefix):].strip()
            break

    if not cmd or not query:
        return

    async with message.channel.typing():
        try:
            if cmd == "check":
                res = await perplexity_check(query)
                return await fast_send(message.channel, f"🔍 {res}")

            if cmd in ("ask", "fast", "search"):
                # Token limit parameter (only for ask)
                token_limit = 2048
                if cmd == "ask":
                    query, token_limit = parse_token_limit(query, 2048)
                    token_limit = max(256, min(4096, token_limit))

                # Attachments
                file_content = ""
                if message.attachments and cmd == "ask":
                    await message.channel.send(f"📎 Processing file attachments (tokens={token_limit} per chunk)…")
                    for attachment in message.attachments:
                        if attachment.size > 10 * 1024 * 1024:  # 10MB
                            await message.channel.send(f"⚠️ File `{attachment.filename}` too large (max 10MB)")
                            continue
                        print(f"📎 Attachment: {attachment.filename} ({attachment.size} bytes)")
                        extracted = await extract_text_from_attachment(attachment, token_limit)
                        if extracted:
                            # If a big analysis was produced, send it directly
                            if "COMPLETE ANALYSIS OF" in extracted and len(extracted) > 2000:
                                await fast_send(message.channel, f"🧠📎 {extracted}")
                            else:
                                file_content += f"\n\n--- Content from {attachment.filename} ---\n{extracted}\n"

                # Build prompt
                ctx = llm.system_msg
                if file_content:
                    ctx += f"\n\nThe user has provided the following file content for analysis:\n{file_content}"
                    query = f"{query}\n\nPlease analyze the provided file content and answer the question accordingly."

                msgs = [{"role": "system", "content": ctx}, {"role": "user", "content": query}]
                resp, _ = llm.generate(msgs, thinking=(cmd == "ask"))
                tag = {"ask": "🧠", "fast": "⚡", "search": "🌐"}.get(cmd, "🤖")
                if file_content and cmd == "ask":
                    tag += f" 📎 (tokens={token_limit})"
                return await fast_send(message.channel, f"{tag} {resp}")

            if cmd == "harvester":
                await message.channel.send(f"🌾 Starting TheHarvester scan for `{query}`…")
                outcome = await osint_manager.run_harvester_scan(query)
                if outcome["status"] != "completed":
                    return await fast_send(message.channel, f"🌾 Error: {outcome.get('error','unknown')}")
                items = outcome["results"]
                if not items:
                    return await fast_send(message.channel, f"🌾 No assets found for `{query}`.")
                by_type = {}
                for item in items:
                    by_type.setdefault(item["type"], []).append(item["value"])
                lines = [f"🌾 **TheHarvester Results for `{query}`:**", f"**Total items found:** {len(items)}", ""]
                for t, vals in by_type.items():
                    lines.append(f"**{t.title()}s:** {len(vals)} found")
                    for v in vals[:5]:
                        lines.append(f"• `{v}`")
                    if len(vals) > 5:
                        lines.append(f"  *...and {len(vals)-5} more*")
                    lines.append("")
                return await fast_send(message.channel, "\n".join(lines))

            if cmd == "spiderfoot":
                await message.channel.send(f"🕷️ Starting SpiderFoot scan for `{query}`…")
                out = await osint_manager.run_spiderfoot_scan(query)
                if out.get("status") != "completed":
                    return await fast_send(message.channel, f"🕷️ SpiderFoot Error: {out.get('error','unknown')}")
                results = out.get("results", [])
                if not results:
                    return await fast_send(message.channel, f"🕷️ No data found for `{query}`.")
                by_type = {}
                for r in results:
                    by_type.setdefault(r.get("type", "unknown"), []).append(r.get("data", ""))
                lines = [f"🕷️ **SpiderFoot Results for `{query}`:**", f"**Total findings:** {len(results)}", ""]
                for et, data_list in sorted(by_type.items())[:10]:
                    clean_type = et.replace("_", " ").title()
                    lines.append(f"**{clean_type}:** {len(data_list)} items")
                    for item in data_list[:3]:
                        if len(item) < 100:
                            lines.append(f"• {item}")
                    if len(data_list) > 3:
                        lines.append(f"  *...and {len(data_list)-3} more*")
                    lines.append("")
                return await fast_send(message.channel, "\n".join(lines))

            if cmd == "sherlock":
                await message.channel.send(f"🔍 Starting Sherlock for `{query}`…")
                out = await osint_manager.run_sherlock_scan(query)
                if out.get("status") != "completed":
                    return await fast_send(message.channel, f"🔍 Error: {out.get('error','unknown')}")
                urls = [r["url"] for r in out["results"]]
                if not urls:
                    return await fast_send(message.channel, f"🔍 No profiles for `{query}`.")
                lines = [f"• {u}" for u in urls]
                return await fast_send(message.channel, "🔍 Sherlock results:\n" + "\n".join(lines))

        except Exception as e:
            await message.channel.send(f"❌ Error: {str(e)}")

if __name__ == "__main__":
    token = os.getenv("DISCORD_TOKEN")
    if not token:
        print("❌ Set DISCORD_TOKEN")
        exit(1)
    print("🚀 Launching bot...")
    try:
        bot.run(token)
    finally:
        cleanup_gpu()
