import os
import discord
import asyncio
import torch
import time
import requests
import gc
import psutil
import json
import re
import glob
import csv
from pathlib import Path
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    logging
)

# â”€â”€â”€ Performance Optimizations â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["MKL_NUM_THREADS"] = "8"
torch.set_num_threads(psutil.cpu_count(logical=False))
logging.set_verbosity_error()

print("ğŸ¤– DeepSeek-R1 High-Performance Discord Bot")
print("=" * 50)

# â”€â”€â”€ System Info â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cpu_cores = psutil.cpu_count(logical=False)
available_ram_gb = psutil.virtual_memory().total / (1024**3)
print(f"ğŸ’» CPU: {cpu_cores} cores | RAM: {available_ram_gb:.1f} GB")

# â”€â”€â”€ Discord Setup â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
intents = discord.Intents.default()
intents.message_content = True
bot = discord.Client(intents=intents)
CHANNEL_ID = DISCORD CHANNEL ID

# Create downloads directory
downloads_dir = Path("downloads")
downloads_dir.mkdir(exist_ok=True)

# â”€â”€â”€ File Processing Functions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def extract_text_from_attachment(attachment):
    """Extract text content from Discord attachment with chunked processing"""
    try:
        file_data = await attachment.read()
        filename = attachment.filename.lower()
        
        print(f"ğŸ“ Processing {filename} ({len(file_data)} bytes)")
        
        # Handle different file types
        if filename.endswith(('.txt', '.md', '.py', '.js', '.html', '.css', '.xml', '.json')):
            text = file_data.decode('utf-8', errors='ignore')
            print(f"ğŸ“ Extracted {len(text)} characters from text file")
            return await process_large_text(text, filename)
            
        elif filename.endswith('.csv'):
            content = file_data.decode('utf-8', errors='ignore')
            print(f"ğŸ“ Extracted {len(content)} characters from CSV")
            return await process_large_text(f"CSV Data:\n{content}", filename)
            
        elif filename.endswith('.pdf'):
            print("ğŸ“ Attempting PDF processing...")
            try:
                import PyPDF2
                import io
                
                print("ğŸ“ PyPDF2 imported successfully")
                pdf_reader = PyPDF2.PdfReader(io.BytesIO(file_data))
                page_count = len(pdf_reader.pages)
                print(f"ğŸ“ PDF has {page_count} pages")
                
                # Extract all text first
                full_text = ""
                for i in range(page_count):
                    try:
                        page = pdf_reader.pages[i]
                        page_text = page.extract_text()
                        full_text += f"\n--- Page {i+1} ---\n{page_text}\n"
                        
                        if i % 10 == 0:  # Progress update every 10 pages
                            print(f"ğŸ“ Processed {i+1}/{page_count} pages")
                            
                    except Exception as page_error:
                        print(f"ğŸ“ Error on page {i+1}: {page_error}")
                        full_text += f"\n--- Page {i+1} (Error) ---\nCould not extract text from this page.\n"
                
                print(f"ğŸ“ Total PDF text extracted: {len(full_text)} characters from {page_count} pages")
                
                if full_text.strip():
                    return await process_large_text(full_text, filename)
                else:
                    return "PDF processed but no readable text found. This might be a scanned document requiring OCR."
                    
            except ImportError as import_error:
                print(f"ğŸ“ PyPDF2 import failed: {import_error}")
                return f"PDF file detected but PyPDF2 not available. Error: {import_error}"
            except Exception as pdf_error:
                print(f"ğŸ“ PDF processing error: {pdf_error}")
                return f"Error reading PDF: {pdf_error}"
                
        # ... other file type handlers remain the same ...
        
    except Exception as e:
        print(f"ğŸ“ General error processing {attachment.filename}: {e}")
        return f"Error processing attachment {attachment.filename}: {str(e)}"

async def process_large_text(full_text: str, filename: str, chunk_size: int = 15000) -> str:
    """Process large text in chunks and combine the analysis"""
    
    if len(full_text) <= chunk_size:
        print(f"ğŸ“ Text is small enough ({len(full_text)} chars), processing normally")
        return full_text
    
    print(f"ğŸ“ Large text detected ({len(full_text):,} chars), processing in chunks of {chunk_size:,} chars")
    
    # Split text into chunks
    chunks = []
    for i in range(0, len(full_text), chunk_size):
        chunk = full_text[i:i + chunk_size]
        # Try to break at sentence boundaries
        if i + chunk_size < len(full_text):
            last_period = chunk.rfind('. ')
            if last_period > chunk_size * 0.7:  # If we can break at 70% or more
                actual_chunk = chunk[:last_period + 1]
                chunks.append(actual_chunk)
                # Adjust next chunk start
                full_text = full_text[:i] + actual_chunk + full_text[i + len(actual_chunk):]
            else:
                chunks.append(chunk)
        else:
            chunks.append(chunk)
    
    total_chunks = len(chunks)
    print(f"ğŸ“ Split into {total_chunks} chunks for processing")
    
    # Process each chunk and get summaries
    chunk_summaries = []
    
    for i, chunk in enumerate(chunks):
        try:
            print(f"ğŸ“ Processing chunk {i+1}/{total_chunks}")
            
            # Create a focused prompt for chunk analysis
            chunk_prompt = f"Analyze this section of the document '{filename}' (Part {i+1} of {total_chunks}) and provide key points, important information, and main concepts. Be concise but comprehensive:\n\n{chunk}"
            
            # Process this chunk with the AI
            messages = [
                {"role": "system", "content": "You are an AI assistant that analyzes document sections and extracts key information. Provide clear, organized summaries of the main points."},
                {"role": "user", "content": chunk_prompt}
            ]
            
            response, _ = llm.generate(messages, thinking=False)  # Use fast mode for chunks
            chunk_summaries.append(f"**Section {i+1}/{total_chunks}:**\n{response}")
            
            print(f"ğŸ“ Completed chunk {i+1}/{total_chunks}")
            
            # Small delay to prevent overwhelming the GPU
            await asyncio.sleep(0.5)
            
        except Exception as chunk_error:
            print(f"ğŸ“ Error processing chunk {i+1}: {chunk_error}")
            chunk_summaries.append(f"**Section {i+1}/{total_chunks}:**\nError processing this section: {str(chunk_error)}")
    
    # Combine all summaries
    combined_summary = f"**COMPLETE ANALYSIS OF '{filename.upper()}'**\n\n"
    combined_summary += f"Document processed in {total_chunks} sections ({len(full_text):,} total characters)\n\n"
    combined_summary += "\n\n".join(chunk_summaries)
    
    print(f"ğŸ“ Completed processing all {total_chunks} chunks")
    return combined_summary

# Update the !ask handler in your on_message function:
if cmd in ("ask", "fast", "search"):
    # Check for file attachments
    file_content = ""
    if message.attachments and cmd == "ask":
        await message.channel.send("ğŸ“ Processing file attachments...")
        
        for attachment in message.attachments:
            if attachment.size > 50 * 1024 * 1024:  # Increased limit to 50MB
                await message.channel.send(f"âš ï¸ File `{attachment.filename}` is too large (max 50MB)")
                continue
                
            print(f"ğŸ“ Processing attachment: {attachment.filename} ({attachment.size} bytes)")
            
            extracted_text = await extract_text_from_attachment(attachment)
            if extracted_text:
                # For large documents, the extracted_text is already the complete analysis
                if len(extracted_text) > 20000 and "COMPLETE ANALYSIS" in extracted_text:
                    # This is a pre-processed large document analysis
                    tag = "ğŸ§ ğŸ“"
                    return await fast_send(message.channel, f"{tag} **Large Document Analysis Complete**\n\n{extracted_text}")
                else:
                    # Normal file processing
                    file_content += f"\n\n--- Content from {attachment.filename} ---\n{extracted_text}\n"
    
    # Build context for AI responses (for smaller files or no files)
    ctx = llm.system_msg
    if file_content:
        ctx += f"\n\nThe user has provided the following file content for analysis:\n{file_content}"
        query = f"{query}\n\nPlease analyze the provided file content and answer the question accordingly."
    
    msgs = [{"role": "system", "content": ctx}, {"role": "user", "content": query}]
    resp, _ = llm.generate(msgs, thinking=(cmd == "ask"))
    tag = {"ask": "ğŸ§ ", "fast": "âš¡", "search": "ğŸŒ"}.get(cmd, "ğŸ¤–")
    
    if file_content:
        file_info = f" ğŸ“ (analyzed {len(message.attachments)} file{'s' if len(message.attachments) > 1 else ''})"
        tag += file_info
    
    return await fast_send(message.channel, f"{tag} {resp}")

def truncate_text(text: str, filename: str, max_chars: int = 20000) -> str:
    """Truncate text to prevent GPU memory issues"""
    if len(text) <= max_chars:
        return text
    
    # Try to find a good breaking point
    truncated = text[:max_chars]
    
    # Try to break at a sentence
    last_period = truncated.rfind('. ')
    if last_period > max_chars * 0.8:  # If we can break at 80% or more
        truncated = truncated[:last_period + 1]
    
    # Add truncation notice
    total_chars = len(text)
    truncated += f"\n\n[NOTE: File '{filename}' truncated from {total_chars:,} to {len(truncated):,} characters to prevent memory issues. This represents approximately {(len(truncated)/total_chars)*100:.1f}% of the original content.]"
    
    print(f"ğŸ“ Truncated {filename} from {total_chars:,} to {len(truncated):,} characters")
    return truncated

# â”€â”€â”€ Response Cleaning â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def clean_llm_response(text: str) -> str:
    if not text:
        return text
    text = re.sub(r'</?think>', '', text, flags=re.IGNORECASE)
    text = re.sub(r'</[^>]*>', '', text)
    sentences = text.split('. ')
    cleaned, seen = [], set()
    for s in sentences:
        s = s.strip()
        if s and s not in seen:
            cleaned.append(s)
            seen.add(s)
    out = '. '.join(cleaned)
    words = out.split()
    if len(words) > 10:
        half = len(words) // 2
        if ' '.join(words[:half]).strip() == ' '.join(words[half:]).strip():
            out = ' '.join(words[:half]).strip()
    return out.strip()

# â”€â”€â”€ Perplexity Config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PERPLEXITY_API_KEY = os.getenv("PERPLEXITY_API_KEY", "API KEY")
PERPLEXITY_URL = "https://api.perplexity.ai/chat/completions"

async def perplexity_check(query: str) -> str:
    headers = {
        "Authorization": f"Bearer {PERPLEXITY_API_KEY}",
        "Content-Type": "application/json"
    }
    payload = {
        "model": "sonar-pro",
        "messages": [{"role": "user", "content": query}],
        "max_tokens": 1000,
        "temperature": 0.2
    }
    loop = asyncio.get_event_loop()
    resp = await asyncio.wait_for(
        loop.run_in_executor(None, lambda: requests.post(PERPLEXITY_URL, headers=headers, json=payload, timeout=30)),
        timeout=35.0
    )
    return resp.json()["choices"][0]["message"]["content"] if resp.status_code == 200 else f"API error {resp.status_code}"

# â”€â”€â”€ GPU Memory Management â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def cleanup_gpu():
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
        gc.collect()

def get_gpu_status():
    if torch.cuda.is_available():
        total = torch.cuda.get_device_properties(0).total_memory / 1024**3
        alloc = torch.cuda.memory_allocated(0) / 1024**3
        return total, alloc, total - alloc
    return 0, 0, 0

# â”€â”€â”€ Model Loading â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
MODEL_PATH = "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B"
print("ğŸ”„ Loading optimized DeepSeek-R1...")
if torch.cuda.is_available():
    print(f"ğŸ’¾ GPU: {torch.cuda.get_device_name(0)} ({torch.cuda.get_device_properties(0).total_memory/1024**3:.1f} GB)")

quant_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_quant_storage=torch.uint8,
)
cleanup_gpu()
print("ğŸ“ Loading tokenizer...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True, use_fast=True)
print("ğŸ”„ Loading and compiling model...")
model = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH,
    quantization_config=quant_config,
    device_map={"": 0},
    trust_remote_code=True,
    torch_dtype=torch.float16,
    max_memory={0: "11GB", "cpu": "20GB"}
)
model.eval()
try:
    compiled_model = torch.compile(model, mode="reduce-overhead")
    print("âœ… Model compiled")
except Exception:
    compiled_model = model
    print("âš ï¸ Compilation not available")
print("ğŸ”¥ Pre-warming modelâ€¦")
dummy = tokenizer("Hello", return_tensors="pt", padding=True).to(compiled_model.device)
with torch.no_grad():
    compiled_model.generate(**dummy, max_new_tokens=5, do_sample=False)
del dummy
total_gpu, alloc_gpu, _ = get_gpu_status()
print(f"âœ… Model ready! GPU: {alloc_gpu:.1f}GB/{total_gpu:.1f}GB")
cleanup_gpu()

# â”€â”€â”€ Inference Wrapper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class OptimizedDeepSeekR1:
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
        self.device = next(model.parameters()).device
        self.max_thinking = 3072
        self.max_fast = 1536
        self.system_msg = (
            "You are DeepSeek R1, an advanced AI assistant. "
            "Provide clear, concise responses without repetition."
        )
        print(f"ğŸ”§ LLM ready | Thinking: {self.max_thinking} | Fast: {self.max_fast}")

    def generate(self, messages, thinking=True):
        start = time.time()
        text = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        inputs = self.tokenizer(text, return_tensors="pt", max_length=32768, truncation=True)
        inputs = {k: v.to(self.device, non_blocking=True) for k, v in inputs.items()}
        gen_kwargs = {
            "max_new_tokens": self.max_thinking if thinking else self.max_fast,
            "do_sample": True,
            "temperature": 0.4 if thinking else 0.6,
            "top_p": 0.9,
            "top_k": 40,
            "pad_token_id": self.tokenizer.eos_token_id,
            "eos_token_id": self.tokenizer.eos_token_id,
            "repetition_penalty": 1.1,
            "use_cache": True,
            "early_stopping": True,
        }
        with torch.no_grad(), torch.amp.autocast("cuda", dtype=torch.float16):
            outputs = self.model.generate(**inputs, **gen_kwargs)
        new_tokens = outputs[0][len(inputs["input_ids"][0]):]
        response = self.tokenizer.decode(new_tokens, skip_special_tokens=True).strip()
        response = clean_llm_response(response)
        tokps = len(new_tokens) / max(1e-6, time.time() - start)
        del outputs, new_tokens, inputs
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        return response, tokps

llm = OptimizedDeepSeekR1(compiled_model, tokenizer)

# â”€â”€â”€ OSINT Manager â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class OSINTToolsManager:
    def __init__(self, base: Path):
        self.base = base
        self.tools = base / "osint_tools"
        self.harv = self.tools / "theHarvester"
        self.sf = self.tools / "spiderfoot"
        self.sh = self.tools / "sherlock"
        self.tools.mkdir(exist_ok=True)

    def create_scan_record(self, target, tool):
        return int(time.time() * 1000)

    def update_scan_status(self, *args):
        pass

    def store_social_profile_data(self, *args):
        pass

    async def run_harvester_scan(self, domain, limit=100):
        """Execute TheHarvester via 'uv run theHarvester -d <domain>'"""
        scan_id = self.create_scan_record(domain, "harvester")
        repo = self.harv
        cmd = ["uv", "run", "theHarvester", "-d", domain, "-l", str(limit)]
        print(f"ğŸŒ¾ Executing TheHarvester: {' '.join(cmd)}")
        
        try:
            proc = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
                cwd=str(repo)
            )
            out, err = await proc.communicate()
            rc = proc.returncode
            text = out.decode(errors="ignore")
            error = err.decode(errors="ignore")
            
            if rc != 0:
                return {"scan_id": scan_id, "status": "failed", "error": error}

            results = []
            # Parse TheHarvester output for different data types
            lines = text.splitlines()
            for line in lines:
                # Look for emails
                if "@" in line and not line.startswith("["):
                    email_match = re.search(r"([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,})", line)
                    if email_match:
                        results.append({"type": "email", "value": email_match.group(1)})
                
                # Look for hostnames/subdomains
                if domain in line and not line.startswith("[") and "." in line:
                    host_match = re.search(r"([a-zA-Z0-9.-]+\." + re.escape(domain.split('.')[-1]) + r")", line)
                    if host_match and host_match.group(1) != domain:
                        results.append({"type": "subdomain", "value": host_match.group(1)})
                
                # Look for IP addresses
                ip_match = re.search(r"\b(\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})\b", line)
                if ip_match:
                    results.append({"type": "ip", "value": ip_match.group(1)})

            # Remove duplicates
            seen = set()
            unique_results = []
            for r in results:
                key = (r["type"], r["value"])
                if key not in seen:
                    seen.add(key)
                    unique_results.append(r)

            return {"scan_id": scan_id, "status": "completed", "results": unique_results}
        
        except Exception as e:
            return {"scan_id": scan_id, "status": "failed", "error": str(e)}

    async def run_spiderfoot_scan(self, target, timeout=300):
        """Run SpiderFoot scan using CLI method"""
        try:
            scan_id = self.create_scan_record(target, "spiderfoot")
            sf_script = self.sf / "sf.py"
            if sf_script.exists():
                cmd = [
                    "python3", str(sf_script),
                    "-s", target,
                    "-u", "all",
                    "-o", "csv",
                    "-q"
                ]
                
                print(f"ğŸ•·ï¸ Executing SpiderFoot: {' '.join(cmd)}")
                proc = await asyncio.create_subprocess_exec(
                    *cmd,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE,
                    cwd=str(self.sf)
                )
                
                stdout, stderr = await asyncio.wait_for(proc.communicate(), timeout=timeout)
                rc = proc.returncode
                s_out = stdout.decode(errors="ignore")
                s_err = stderr.decode(errors="ignore")
                
                print(f"[SpiderFoot rc={rc}]")
                if s_err:
                    print(f"SpiderFoot stderr: {s_err[:500]}")
                
                results = []
                if s_out and rc == 0:
                    lines = s_out.strip().split('\n')
                    for line in lines[1:]:
                        if line.strip():
                            try:
                                parts = line.split('\t')
                                if len(parts) >= 5:
                                    event_type = parts[2]
                                    data = parts[4]
                                    results.append({
                                        "type": event_type,
                                        "data": data,
                                        "target": target
                                    })
                            except:
                                continue
                
                if not results and rc != 0:
                    return {"scan_id": scan_id, "status": "failed", "error": s_err or "SpiderFoot scan failed"}
                
                return {"scan_id": scan_id, "status": "completed", "results": results[:50]}
            else:
                return {"scan_id": scan_id, "status": "failed", "error": "SpiderFoot sf.py not found"}
                
        except asyncio.TimeoutError:
            return {"scan_id": scan_id, "status": "failed", "error": f"SpiderFoot scan timed out after {timeout}s"}
        except Exception as e:
            return {"scan_id": scan_id, "status": "failed", "error": str(e)}

    async def run_sherlock_scan(self, username, timeout=60):
        scan_id = self.create_scan_record(username, "sherlock")
        exe = self.sh / "sherlock" / "Scripts" / "sherlock.exe"
        cwd = str(self.sh / "sherlock")
        if exe.exists():
            cmd = [str(exe), username, "--print-found", "--timeout", str(timeout), "--csv"]
        else:
            cmd = ["python", "-m", "sherlock", username, "--print-found", "--timeout", str(timeout), "--csv"]
            cwd = str(self.sh)
        
        print("Exec:", *cmd)
        proc = await asyncio.create_subprocess_exec(
            *cmd,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE,
            cwd=cwd
        )
        out, err = await proc.communicate()
        rc = proc.returncode
        s_out = out.decode(errors="ignore")
        s_err = err.decode(errors="ignore")
        print(f"[Sherlock rc={rc}]")
        
        results = []
        for fpath in glob.glob(os.path.join(cwd, f"*{username}*.csv")):
            with open(fpath, newline="", encoding="utf-8", errors="ignore") as fh:
                reader = csv.reader(fh)
                for row in reader:
                    for col in row:
                        if col.startswith(("http://", "https://")):
                            results.append({"type": "social_profile", "url": col})
            try: os.remove(fpath)
            except: pass
        
        if not results:
            for line in s_out.splitlines():
                m = re.search(r"(https?://\S+)", line)
                if m:
                    results.append({"type": "social_profile", "url": m.group(1)})
        
        if rc != 0 and not results:
            return {"scan_id": scan_id, "status": "failed", "error": s_err or "Sherlock failed"}
        
        return {"scan_id": scan_id, "status": "completed", "results": results}

osint_manager = OSINTToolsManager(Path.cwd())

# â”€â”€â”€ Helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def fast_send(ch, txt):
    if not txt:
        return await ch.send("No response.")
    txt = clean_llm_response(txt)
    chunks = []
    while txt:
        c = txt[:1900]
        if len(txt) > 1900:
            i = c.rfind(". ")
            if i > 950:
                c = c[: i + 1]
        chunks.append(c)
        txt = txt[len(c):].strip()
    for idx, c in enumerate(chunks):
        await ch.send(c)
        if idx < len(chunks) - 1:
            await asyncio.sleep(0.2)

def quick_needs_search(q):
    return any(w in q.lower() for w in ["current", "latest", "news", "who", "what", "when", "where", "202"])

# â”€â”€â”€ Bot Events â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@bot.event
async def on_ready():
    print("âœ… Bot online:", bot.user)
    channel = bot.get_channel(CHANNEL_ID)
    if not channel:
        print(f"âŒ Channel {CHANNEL_ID} not found.")
        return
    gpu, alloc, _ = get_gpu_status()
    
    # Split into multiple shorter messages
    startup_message_1 = (
        f"ğŸš€ **DeepSeek-R1 High-Performance Discord Bot Ready!**\n\n"
        f"**System Status:**\n"
        f"â€¢ GPU: {alloc:.1f}GB/{gpu:.1f}GB | Compiled: âœ…\n"
        f"â€¢ Features: Perplexity, OSINT Scans, File Analysis\n\n"
        "**ğŸ“‹ Available Commands:**\n\n"
        "**ğŸ§  AI & Search Commands:**\n"
        "â€¢ `!ask <question>` - Deep reasoning mode (supports file attachments)\n"
        "â€¢ `!fast <question>` - Quick response mode\n"
        "â€¢ `!search <query>` - Web search + AI analysis\n"
        "â€¢ `!check <query>` - Direct Perplexity API lookup\n\n"
        "**ğŸ“ File Analysis:**\n"
        "â€¢ Attach files to `!ask` commands for analysis\n"
        "â€¢ **Supported:** .txt, .md, .py, .js, .html, .css, .xml, .json, .csv, .pdf, .docx, .doc, .xlsx, .xls\n"
        "â€¢ **Usage:** Upload a file and use `!ask analyze this document`"
    )
    
    startup_message_2 = (
        "**ğŸ” OSINT Reconnaissance Tools:**\n\n"
        "**ğŸŒ¾ TheHarvester - Email & Domain Intelligence**\n"
        "â€¢ `!harvester <domain.com>` - Domain reconnaissance\n"
        "â€¢ **Finds:** Email addresses, subdomains, IP addresses, hostnames\n"
        "â€¢ **Sources:** Google, Bing, LinkedIn, Twitter, and 40+ more\n\n"
        "**ğŸ•·ï¸ SpiderFoot - Advanced OSINT Automation**\n"
        "â€¢ `!spiderfoot <domain.com>` - Multi-module intelligence gathering\n"
        "â€¢ **Finds:** DNS records, SSL certs, social media, vulnerabilities\n"
        "â€¢ **Modules:** 200+ reconnaissance modules\n\n"
        "**ğŸ” Sherlock - Social Media Username Hunter**\n"
        "â€¢ `!sherlock <username>` - Hunt usernames across 350+ platforms\n"
        "â€¢ **Finds:** Active social media profiles, account verification\n"
        "â€¢ **Platforms:** GitHub, Instagram, Twitter, Reddit, TikTok, LinkedIn, and 340+ more\n\n"
        "**Ready for reconnaissance operations!** ğŸ¯"
    )
    
    try:
        await channel.send(startup_message_1)
        await asyncio.sleep(1)  # Small delay between messages
        await channel.send(startup_message_2)
        print("âœ… Startup messages sent successfully to Discord!")
    except Exception as e:
        print(f"âŒ Error sending startup messages: {e}")

@bot.event
async def on_message(message):
    if message.author == bot.user or message.channel.id != CHANNEL_ID:
        return
    
    content = message.content.strip()
    cmd, query = None, None
    for key, prefix in [
        ("ask", "!ask "), ("fast", "!fast "), ("search", "!search "),
        ("check", "!check "), ("harvester", "!harvester "),
        ("spiderfoot", "!spiderfoot "), ("sherlock", "!sherlock ")
    ]:
        if content.startswith(prefix):
            cmd, query = key, content[len(prefix):].strip()
            break
    
    if not cmd or not query:
        return

    async with message.channel.typing():
        try:
            if cmd == "check":
                res = await perplexity_check(query)
                return await fast_send(message.channel, f"ğŸ” {res}")

            # â”€â”€â”€ Enhanced !ask Handler with File Support â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            if cmd in ("ask", "fast", "search"):
                # Check for file attachments
                file_content = ""
                if message.attachments and cmd == "ask":
                    await message.channel.send("ğŸ“ Processing file attachments...")
                    
                    for attachment in message.attachments:
                        # Check file size (max 25MB for Discord, but we'll limit to 10MB for processing)
                        if attachment.size > 10 * 1024 * 1024:  # 10MB
                            await message.channel.send(f"âš ï¸ File `{attachment.filename}` is too large (max 10MB)")
                            continue
                            
                        print(f"ğŸ“ Processing attachment: {attachment.filename} ({attachment.size} bytes)")
                        
                        extracted_text = await extract_text_from_attachment(attachment)
                        if extracted_text:
                            file_content += f"\n\n--- Content from {attachment.filename} ---\n{extracted_text}\n"
                
                # Build context for AI responses
                ctx = llm.system_msg
                if file_content:
                    ctx += f"\n\nThe user has provided the following file content for analysis:\n{file_content}"
                    query = f"{query}\n\nPlease analyze the provided file content and answer the question accordingly."
                
                if cmd in ("search", "ask") and quick_needs_search(query):
                    # Web search integration if needed (removed DuckDuckGo dependency)
                    pass
                
                msgs = [{"role": "system", "content": ctx}, {"role": "user", "content": query}]
                resp, _ = llm.generate(msgs, thinking=(cmd == "ask"))
                tag = {"ask": "ğŸ§ ", "fast": "âš¡", "search": "ğŸŒ"}.get(cmd, "ğŸ¤–")
                
                if file_content:
                    file_info = f" ğŸ“ (analyzed {len(message.attachments)} file{'s' if len(message.attachments) > 1 else ''})"
                    tag += file_info
                
                return await fast_send(message.channel, f"{tag} {resp}")

            # â”€â”€â”€ TheHarvester Handler â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            if cmd == "harvester":
                await message.channel.send(f"ğŸŒ¾ Starting TheHarvester scan for `{query}`â€¦")
                outcome = await osint_manager.run_harvester_scan(query)
                if outcome["status"] != "completed":
                    return await fast_send(message.channel, f"ğŸŒ¾ Error: {outcome.get('error','unknown')}")
                
                items = outcome["results"]
                if not items:
                    return await fast_send(message.channel, f"ğŸŒ¾ No assets found for `{query}`.")
                
                # Group results by type
                by_type = {}
                for item in items:
                    item_type = item["type"]
                    if item_type not in by_type:
                        by_type[item_type] = []
                    by_type[item_type].append(item["value"])
                
                summary_lines = [f"ğŸŒ¾ **TheHarvester Results for `{query}`:**"]
                summary_lines.append(f"**Total items found:** {len(items)}")
                summary_lines.append("")
                
                for item_type, values in by_type.items():
                    summary_lines.append(f"**{item_type.title()}s:** {len(values)} found")
                    for value in values[:5]:  # Show first 5 items
                        summary_lines.append(f"â€¢ `{value}`")
                    if len(values) > 5:
                        summary_lines.append(f"  *...and {len(values) - 5} more*")
                    summary_lines.append("")
                
                return await fast_send(message.channel, "\n".join(summary_lines))

            # â”€â”€â”€ SpiderFoot Handler â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            if cmd == "spiderfoot":
                await message.channel.send(f"ğŸ•·ï¸ Starting SpiderFoot scan for `{query}`â€¦")
                out = await osint_manager.run_spiderfoot_scan(query)
                if out.get("status") != "completed":
                    return await fast_send(message.channel, f"ğŸ•·ï¸ SpiderFoot Error: {out.get('error','unknown')}")
                
                results = out.get("results", [])
                if not results:
                    return await fast_send(message.channel, f"ğŸ•·ï¸ No data found for `{query}`.")
                
                by_type = {}
                for r in results:
                    event_type = r.get("type", "unknown")
                    if event_type not in by_type:
                        by_type[event_type] = []
                    by_type[event_type].append(r.get("data", ""))
                
                summary_lines = [f"ğŸ•·ï¸ **SpiderFoot Results for `{query}`:**"]
                summary_lines.append(f"**Total findings:** {len(results)}")
                summary_lines.append("")
                
                for event_type, data_list in sorted(by_type.items())[:10]:
                    clean_type = event_type.replace("_", " ").title()
                    summary_lines.append(f"**{clean_type}:** {len(data_list)} items")
                    for item in data_list[:3]:
                        if len(item) < 100:
                            summary_lines.append(f"â€¢ {item}")
                    if len(data_list) > 3:
                        summary_lines.append(f"  *...and {len(data_list) - 3} more*")
                    summary_lines.append("")
                
                return await fast_send(message.channel, "\n".join(summary_lines))

            # â”€â”€â”€ Sherlock Handler â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            if cmd == "sherlock":
                await message.channel.send(f"ğŸ” Starting Sherlock for `{query}`â€¦")
                out = await osint_manager.run_sherlock_scan(query)
                if out.get("status") != "completed":
                    return await fast_send(message.channel, f"ğŸ” Error: {out.get('error','unknown')}")
                urls = [r["url"] for r in out["results"]]
                if not urls:
                    return await fast_send(message.channel, f"ğŸ” No profiles for `{query}`.")
                lines = [f"â€¢ {u}" for u in urls]
                return await fast_send(message.channel, "ğŸ” Sherlock results:\n" + "\n".join(lines))

        except Exception as e:
            await message.channel.send(f"âŒ Error: {str(e)}")

if __name__ == "__main__":
    token = os.getenv("DISCORD_TOKEN")
    if not token:
        print("âŒ Set DISCORD_TOKEN")
        exit(1)
    print("ğŸš€ Launching bot...")
    try:
        bot.run(token)
    finally:
        cleanup_gpu()
